{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f37288-ea52-45b3-a695-035f9cde0f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ•°æ®...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_974/1683348422.py:118: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  close_feats = close_prices.groupby('ISIN').apply(extract_close_features).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¸…æ´—ä¸ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆã€‚\n",
      "å¯æ˜ å°„äº¤æ˜“æ•°: 228913, ç¼ºå¤±user_idx: 0, ç¼ºå¤±asset_idx: 0\n",
      "æ„å»ºæ­£æ ·æœ¬...\n",
      "æ­£æ ·æœ¬: 89884, è´Ÿæ ·æœ¬: 359536\n",
      "Train æ ·æœ¬: 359536, Valid æ ·æœ¬: 89884\n",
      "DataLoader æ„å»ºå®Œæˆï¼štrain 703 æ‰¹æ¬¡ï¼Œvalid 176 æ‰¹æ¬¡\n",
      "Step 1 å®Œæˆã€‚å¯è¿›å…¥æ¨¡å‹å®šä¹‰é˜¶æ®µã€‚\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# xDeepFM å¤ç°é¡¹ç›® - Step 0 + Step 1\n",
    "# æ•°æ®é¢„å¤„ç† + è´Ÿé‡‡æ · + è®­ç»ƒé›†æ„å»º\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -----------------------\n",
    "# åŸºç¡€é…ç½®\n",
    "# -----------------------\n",
    "data_path = \"/workspace/FAR-Trans/FAR-Trans//\"\n",
    "FILES = {\n",
    "    'customers': os.path.join(data_path, \"customer_information.csv\"),\n",
    "    'assets': os.path.join(data_path, \"asset_information.csv\"),\n",
    "    'transactions': os.path.join(data_path, \"transactions.csv\"),\n",
    "    'markets': os.path.join(data_path, \"markets.csv\"),\n",
    "    'limit_prices': os.path.join(data_path, \"limit_prices.csv\"),\n",
    "    'close_prices': os.path.join(data_path, \"close_prices.csv\"),\n",
    "}\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 512\n",
    "NEG_SAMPLE_PER_POS = 4\n",
    "DYNAMIC_NEG = False  # â—å·²å…³é—­ï¼šä¸å†è¿›è¡ŒåŠ¨æ€è´Ÿé‡‡æ ·ï¼ˆæœ€åˆç‰ˆæœ¬æ˜¯ç”¨åŠ¨æ€è´Ÿé‡‡æ ·ï¼Œaucè™šé«˜ï¼Œåè½¬å˜ä¸ºéš¾è´Ÿé‡‡æ ·ï¼‰\n",
    "\n",
    "# ---- è®¾å¤‡ä¿¡æ¯ï¼ˆä»…ç”¨äº DataLoader ç¼ºçœè®¾å®šï¼‰\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IS_WINDOWS = (platform.system() == \"Windows\")\n",
    "\n",
    "# ---- éšæœºæ€§ä¸ç¨³å®šæ€§\n",
    "def set_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        import torch.backends.cudnn as cudnn\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "set_seeds(SEED)\n",
    "\n",
    "# -----------------------\n",
    "# Step 0: æ•°æ®é¢„å¤„ç†\n",
    "# -----------------------\n",
    "print(\"åŠ è½½æ•°æ®...\")\n",
    "customers = pd.read_csv(FILES['customers'])\n",
    "assets = pd.read_csv(FILES['assets'])\n",
    "transactions = pd.read_csv(FILES['transactions'])\n",
    "markets = pd.read_csv(FILES['markets'])\n",
    "limit_prices = pd.read_csv(FILES['limit_prices'])\n",
    "close_prices = pd.read_csv(FILES['close_prices'])\n",
    "close_prices['timestamp'] = pd.to_datetime(close_prices['timestamp'])\n",
    "\n",
    "# ä»…ä¿ç•™ Buy äº¤æ˜“\n",
    "transactions = transactions[transactions['transactionType'] == 'Buy']\n",
    "\n",
    "# ç”¨æˆ·ä¸èµ„äº§æœ€æ–°è®°å½•\n",
    "customers_latest = customers.sort_values('timestamp').groupby('customerID').tail(1)\n",
    "assets_latest = assets.sort_values('timestamp').groupby('ISIN').tail(1)\n",
    "\n",
    "# èµ„äº§æ‰©å±•ç‰¹å¾\n",
    "assets_latest = assets_latest.merge(\n",
    "    markets[['marketID', 'country', 'marketClass', 'tradingHours']],\n",
    "    on='marketID', how='left'\n",
    ")\n",
    "assets_latest[['country', 'marketClass', 'tradingHours']] = (\n",
    "    assets_latest[['country', 'marketClass', 'tradingHours']].fillna('Unknown')\n",
    ")\n",
    "\n",
    "# limit_prices\n",
    "assets_latest = assets_latest.merge(\n",
    "    limit_prices[['ISIN', 'profitability', 'priceMinDate', 'priceMaxDate', 'minDate', 'maxDate']],\n",
    "    on='ISIN', how='left'\n",
    ")\n",
    "\n",
    "# æ•°å€¼å¤„ç†\n",
    "assets_latest['priceMinDate'] = pd.to_numeric(assets_latest['priceMinDate'], errors='coerce')\n",
    "assets_latest['priceMaxDate'] = pd.to_numeric(assets_latest['priceMaxDate'], errors='coerce')\n",
    "assets_latest['price_volatility_ratio'] = np.where(\n",
    "    assets_latest['priceMinDate'] != 0,\n",
    "    (assets_latest['priceMaxDate'] - assets_latest['priceMinDate']) / assets_latest['priceMinDate'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "assets_latest['minDate'] = pd.to_datetime(assets_latest['minDate'], errors='coerce')\n",
    "assets_latest['maxDate'] = pd.to_datetime(assets_latest['maxDate'], errors='coerce')\n",
    "assets_latest['listing_days'] = (assets_latest['maxDate'] - assets_latest['minDate']).dt.days\n",
    "\n",
    "# close_prices èšåˆ\n",
    "def extract_close_features(group):\n",
    "    group = group.sort_values('timestamp')\n",
    "    prices = group['closePrice'].values\n",
    "    if len(prices) < 2:\n",
    "        return pd.Series({'price_mean': np.nan, 'price_stability': np.nan,\n",
    "                          'max_drawdown': np.nan, 'recent_return': np.nan})\n",
    "    price_mean = prices.mean()\n",
    "    price_std = prices.std()\n",
    "    stability = price_std / price_mean if price_mean else np.nan\n",
    "    cum_max = np.maximum.accumulate(prices)\n",
    "    drawdowns = (prices / cum_max) - 1\n",
    "    max_dd = drawdowns.min()\n",
    "    recent_return = (prices[-1] - prices[max(-30, -len(prices))]) / prices[max(-30, -len(prices))]\n",
    "    return pd.Series({'price_mean': price_mean, 'price_stability': stability,\n",
    "                      'max_drawdown': max_dd, 'recent_return': recent_return})\n",
    "\n",
    "close_feats = close_prices.groupby('ISIN').apply(extract_close_features).reset_index()\n",
    "assets_latest = assets_latest.merge(close_feats, on='ISIN', how='left')\n",
    "\n",
    "# æ•°å€¼ç‰¹å¾å¤„ç†\n",
    "num_cols = ['profitability', 'price_volatility_ratio', 'listing_days',\n",
    "            'price_mean', 'price_stability', 'max_drawdown', 'recent_return']\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "assets_latest[num_cols] = imputer.fit_transform(assets_latest[num_cols])\n",
    "\n",
    "assets_latest['listing_days'] = np.log1p(assets_latest['listing_days'].clip(lower=0))\n",
    "assets_latest['recent_return'] = assets_latest['recent_return'].clip(-1, 1)\n",
    "assets_latest['max_drawdown'] = assets_latest['max_drawdown'].clip(-1, 0)\n",
    "assets_latest['price_volatility_ratio'] = assets_latest['price_volatility_ratio'].clip(0, 5)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "assets_latest[num_cols] = scaler.fit_transform(assets_latest[num_cols])\n",
    "\n",
    "# ç±»åˆ«ç¼–ç \n",
    "def encode_cols(df, cols):\n",
    "    encoders = {}\n",
    "    for c in cols:\n",
    "        df[c] = df[c].fillna('Unknown')\n",
    "        le = LabelEncoder()\n",
    "        df[c] = le.fit_transform(df[c].astype(str))\n",
    "        encoders[c] = le\n",
    "    return encoders\n",
    "\n",
    "user_cat_features = ['customerType', 'riskLevel', 'investmentCapacity']\n",
    "user_encoders = encode_cols(customers_latest, user_cat_features)\n",
    "\n",
    "asset_cat_features = ['assetCategory', 'assetSubCategory', 'marketID',\n",
    "                      'sector', 'industry', 'country', 'marketClass', 'tradingHours']\n",
    "asset_encoders = encode_cols(assets_latest, asset_cat_features)\n",
    "\n",
    "# ç”¨æˆ·ä¸èµ„äº§ç´¢å¼•\n",
    "user2idx = {u: i for i, u in enumerate(customers_latest['customerID'].unique())}\n",
    "asset2idx = {a: i for i, a in enumerate(assets_latest['ISIN'].unique())}\n",
    "customers_latest['user_idx'] = customers_latest['customerID'].map(user2idx)\n",
    "assets_latest['asset_idx'] = assets_latest['ISIN'].map(asset2idx)\n",
    "\n",
    "print(\"æ¸…æ´—ä¸ç‰¹å¾æ ‡å‡†åŒ–å®Œæˆã€‚\")\n",
    "\n",
    "# -----------------------\n",
    "# Step 1: æ ·æœ¬æ„å»º + è´Ÿé‡‡æ · + æ•°æ®åˆ’åˆ†ï¼ˆéš¾è´Ÿé‡‡æ ·ï¼šåŒå¸‚åœºä¼˜å…ˆ + å…¨å±€å°‘é‡ï¼‰\n",
    "# -----------------------\n",
    "# æ˜ å°„ & è¿‡æ»¤\n",
    "transactions = transactions[\n",
    "    transactions['customerID'].isin(user2idx.keys()) &\n",
    "    transactions['ISIN'].isin(asset2idx.keys())\n",
    "].copy()\n",
    "transactions['user_idx'] = transactions['customerID'].map(user2idx)\n",
    "transactions['asset_idx'] = transactions['ISIN'].map(asset2idx)\n",
    "\n",
    "missing_users = transactions['user_idx'].isna().sum()\n",
    "missing_assets = transactions['asset_idx'].isna().sum()\n",
    "print(f\"å¯æ˜ å°„äº¤æ˜“æ•°: {len(transactions)}, ç¼ºå¤±user_idx: {missing_users}, ç¼ºå¤±asset_idx: {missing_assets}\")\n",
    "\n",
    "transactions = transactions.dropna(subset=['user_idx', 'asset_idx'])\n",
    "transactions['user_idx'] = transactions['user_idx'].astype(int)\n",
    "transactions['asset_idx'] = transactions['asset_idx'].astype(int)\n",
    "\n",
    "print(\"æ„å»ºæ­£æ ·æœ¬...\")\n",
    "pos_pairs = transactions[['user_idx', 'asset_idx']].drop_duplicates().copy()\n",
    "pos_pairs['label'] = 1\n",
    "\n",
    "num_users = len(user2idx)\n",
    "num_items = len(asset2idx)\n",
    "_rng = random.Random(SEED)  # å±€éƒ¨éšæœºæº\n",
    "\n",
    "# ====== ç¡¬è´Ÿé‡‡æ ·éœ€è¦çš„å¸‚åœºç´¢å¼•ï¼ˆæ¥è‡ªèµ„äº§ä¿¡æ¯æ–‡ä»¶ï¼‰ ======\n",
    "MARKET_RATIO = 0.9   # åŒå¸‚åœºæ¯”ä¾‹\n",
    "GLOBAL_RATIO = 0.1   # å…¨å±€éšæœºå°‘é‡æ¯”ä¾‹ï¼ˆè¡¥é½ï¼‰\n",
    "assert abs(MARKET_RATIO + GLOBAL_RATIO - 1.0) < 1e-6\n",
    "\n",
    "_asset_meta = assets_latest[['asset_idx', 'marketID']].dropna().copy()\n",
    "_asset_meta['asset_idx'] = _asset_meta['asset_idx'].astype(int)\n",
    "_asset_meta['marketID']  = _asset_meta['marketID'].astype(int)\n",
    "\n",
    "from collections import defaultdict\n",
    "_assets_by_market = defaultdict(set)\n",
    "for _, r in _asset_meta.iterrows():\n",
    "    _assets_by_market[int(r['marketID'])].add(int(r['asset_idx']))\n",
    "\n",
    "asset2market = dict(zip(_asset_meta['asset_idx'].tolist(), _asset_meta['marketID'].tolist()))\n",
    "_all_items_set = set(range(num_items))  # æ‰€æœ‰å¯ç”¨èµ„äº§ç´¢å¼•\n",
    "\n",
    "def _split_counts(total_needed: int, market_ratio: float):\n",
    "    mkt = int(round(total_needed * market_ratio))\n",
    "    rnd = total_needed - mkt\n",
    "    return mkt, rnd\n",
    "\n",
    "# ====== éš¾è´Ÿé‡‡æ ·ï¼ˆåŒå¸‚åœºä¼˜å…ˆ + å…¨å±€è¡¥é½ï¼›ä¸¥æ ¼è´Ÿä¾‹ï¼‰ ======\n",
    "def negative_sampling(pos_df, num_users, num_items, neg_per_pos=NEG_SAMPLE_PER_POS, rng_obj=_rng):\n",
    "    pos_df = pos_df.astype({'user_idx': int, 'asset_idx': int})\n",
    "    user_pos = pos_df.groupby('user_idx')['asset_idx'].apply(set).to_dict()\n",
    "    neg_rows = []\n",
    "\n",
    "    for u, pos_set in user_pos.items():\n",
    "        if not pos_set:\n",
    "            continue\n",
    "\n",
    "        global_pool = list(_all_items_set - pos_set)\n",
    "        if not global_pool:\n",
    "            continue\n",
    "\n",
    "        for a in pos_set:\n",
    "            need_total = max(1, int(neg_per_pos))\n",
    "            need_mkt, _ = _split_counts(need_total, MARKET_RATIO)\n",
    "\n",
    "            mk_id = asset2market.get(int(a), None)\n",
    "            mk_pool = list(((_assets_by_market.get(mk_id, set())) & _all_items_set) - pos_set) if mk_id is not None else []\n",
    "\n",
    "            # å…ˆé‡‡åŒå¸‚åœº\n",
    "            take_mk = []\n",
    "            if need_mkt > 0 and mk_pool:\n",
    "                k = min(need_mkt, len(mk_pool))\n",
    "                take_mk = rng_obj.sample(mk_pool, k)\n",
    "\n",
    "            # ç”¨å…¨å±€è¡¥é½ï¼ˆå‰”é™¤å·²äº¤äº’ + å·²é€‰ï¼‰\n",
    "            already = set(take_mk) | pos_set\n",
    "            rnd_pool = list(set(global_pool) - already)\n",
    "            left = need_total - len(take_mk)\n",
    "            take_rnd = []\n",
    "            if left > 0 and rnd_pool:\n",
    "                k = min(left, len(rnd_pool))\n",
    "                take_rnd = rng_obj.sample(rnd_pool, k)\n",
    "\n",
    "            sampled = take_mk + take_rnd\n",
    "\n",
    "            # è‹¥ä»ä¸è¶³ï¼ˆæç«¯å°é›†åˆï¼‰ï¼Œå†ä»å…¨å±€è¡¥\n",
    "            if len(sampled) < need_total:\n",
    "                extra_pool = list(set(global_pool) - set(sampled))\n",
    "                if extra_pool:\n",
    "                    k = min(need_total - len(sampled), len(extra_pool))\n",
    "                    sampled += rng_obj.sample(extra_pool, k)\n",
    "\n",
    "            neg_rows.extend([(u, i) for i in sampled])\n",
    "\n",
    "    neg_df = pd.DataFrame(neg_rows, columns=['user_idx', 'asset_idx'])\n",
    "    neg_df['label'] = 0\n",
    "    return neg_df\n",
    "\n",
    "def build_train_dataframe(pos_df, num_users, num_items, neg_per_pos=NEG_SAMPLE_PER_POS):\n",
    "    neg_df = negative_sampling(pos_df, num_users, num_items, neg_per_pos)\n",
    "    data_df = pd.concat([pos_df, neg_df], axis=0, ignore_index=True)\n",
    "    data_df = data_df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "    return data_df\n",
    "\n",
    "data_df = build_train_dataframe(pos_pairs, num_users, num_items, NEG_SAMPLE_PER_POS)\n",
    "print(f\"æ­£æ ·æœ¬: {len(pos_pairs)}, è´Ÿæ ·æœ¬: {len(data_df) - len(pos_pairs)}\")\n",
    "\n",
    "user_feat_df = customers_latest.set_index('user_idx')[user_cat_features].copy()\n",
    "asset_feat_df = assets_latest.set_index('asset_idx')[asset_cat_features + num_cols].copy()\n",
    "\n",
    "data_df = (\n",
    "    data_df\n",
    "    .merge(user_feat_df, left_on='user_idx', right_index=True, how='left')\n",
    "    .merge(asset_feat_df, left_on='asset_idx', right_index=True, how='left')\n",
    ")\n",
    "\n",
    "assert data_df[user_cat_features + asset_cat_features + num_cols].isna().sum().sum() == 0, \\\n",
    "    \"ä»æœ‰ç¼ºå¤±å€¼ï¼Œè¯·æ£€æŸ¥é¢„å¤„ç†ã€‚\"\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    data_df,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    "    stratify=data_df['label']\n",
    ")\n",
    "print(f\"Train æ ·æœ¬: {len(train_df)}, Valid æ ·æœ¬: {len(valid_df)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Dataset / DataLoader\n",
    "# -----------------------\n",
    "class FARTransDataset(Dataset):\n",
    "    def __init__(self, df, user_cat_cols, asset_cat_cols, asset_num_cols):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.user_cat_cols = user_cat_cols\n",
    "        self.asset_cat_cols = asset_cat_cols\n",
    "        self.asset_num_cols = asset_num_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        user_idx = torch.tensor(row['user_idx'], dtype=torch.long)\n",
    "        asset_idx = torch.tensor(row['asset_idx'], dtype=torch.long)\n",
    "        user_cat = torch.tensor(row[self.user_cat_cols].values.astype(np.int64))\n",
    "        asset_cat = torch.tensor(row[self.asset_cat_cols].values.astype(np.int64))\n",
    "        asset_num = torch.tensor(row[self.asset_num_cols].values.astype(np.float32))\n",
    "        label = torch.tensor(row['label'], dtype=torch.float32)\n",
    "        return user_idx, user_cat, asset_idx, asset_cat, asset_num, label\n",
    "\n",
    "# ---- é€šç”¨ DataLoader å·¥å‚ï¼ˆGPU å‹å¥½ï¼ŒCPU ä¿å®ˆï¼‰\n",
    "def make_loader(dataset, batch_size, shuffle,\n",
    "                num_workers: int = 0,\n",
    "                pin_memory: bool = False,\n",
    "                persistent_workers: bool = False,\n",
    "                prefetch_factor: int | None = None):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=persistent_workers if num_workers > 0 else False,\n",
    "        prefetch_factor=(prefetch_factor if (prefetch_factor and num_workers > 0) else None),\n",
    "    )\n",
    "\n",
    "# ä¿æŒåŸè¡Œä¸ºï¼ˆCPU ä¿å®ˆå‚æ•°ï¼‰\n",
    "train_dataset = FARTransDataset(train_df, user_cat_features, asset_cat_features, num_cols)\n",
    "valid_dataset = FARTransDataset(valid_df, user_cat_features, asset_cat_features, num_cols)\n",
    "train_loader = make_loader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "valid_loader = make_loader(valid_dataset, BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"DataLoader æ„å»ºå®Œæˆï¼štrain {len(train_loader)} æ‰¹æ¬¡ï¼Œvalid {len(valid_loader)} æ‰¹æ¬¡\")\n",
    "\n",
    "# ç›¸è¾ƒåˆç‰ˆå·²åˆ é™¤ï¼šåŠ¨æ€è´Ÿé‡‡æ ·æŒ‚é’©ï¼ˆrebuild_train_loader_for_epochï¼‰ä¸ POS_PAIRS_TRAIN\n",
    "print(\"Step 1 å®Œæˆã€‚å¯è¿›å…¥æ¨¡å‹å®šä¹‰é˜¶æ®µã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9fed547-3226-4790-83c1-806962074032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== xDeepFM æ¨¡å‹æ„å»ºå®Œæˆ ====\n",
      "è®¾å¤‡: cuda\n",
      "Embed ç»´åº¦: 16\n",
      "Field æ•°é‡: 2(ID) + 3(UserCat) + 8(AssetCat) + 7(AssetNum) = 20\n",
      "CIN å±‚: [16, 16] -> è¾“å‡ºç»´åº¦ 32\n",
      "DNN ç»“æ„: [128, 64]\n",
      "å¯è®­ç»ƒå‚æ•°é‡: 543,905\n"
     ]
    }
   ],
   "source": [
    "## step2ç¬¬äºŒæ¬¡è°ƒæ•´\n",
    "# ================================\n",
    "# Step 2: çœŸå® 2 å±‚ CIN + xDeepFM æ¨¡å‹ï¼ˆç¨³å®šå¢å¼ºç‰ˆï¼‰\n",
    "# - CIN æ¯å±‚ BatchNorm1d + GELUï¼ˆé˜²é€šé“çˆ†ç‚¸ï¼‰\n",
    "# - DNN: Linear + LayerNorm + GELU + Dropoutï¼ˆå¯¹ç¨€ç–æ›´ç¨³ï¼‰\n",
    "# - EmbeddingDropout é»˜è®¤ 0.05ï¼ˆå¯å…³ï¼‰\n",
    "# - è¾“å‡ºå±‚ bias å¯ç”¨æ­£ä¾‹åŸºå‡†ç‡åˆå§‹åŒ–ï¼ˆå†·å¯åŠ¨æ›´ç¨³ï¼‰\n",
    "# - logits ææ€§å¼€å…³ logit_signï¼ŒAUC<0.5 æ—¶å¯ä¸€è¡Œç¿»è½¬\n",
    "# ================================\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ===== ç¨³å®šé»˜è®¤è¶…å‚ï¼ˆå¯¹é½å‹å¥½ + å®¹é‡æ”¶æ•›ï¼‰ =====\n",
    "EMBED_DIM  = 16          # 16 ä¾¿äº bf16/fp16 å¯¹é½\n",
    "CIN_SIZES  = [16, 16]    # ç¨³èµ·æ­¥ï¼Œç¨³å®šåå¯å‡åˆ° [32, 32]\n",
    "DNN_HIDDEN = [128, 64]\n",
    "DROPOUT    = 0.35\n",
    "EMB_DROPOUT_DEFAULT = 0.10  # è½»åº¦éšæœºåŒ– embï¼Œæ›´ç¨³\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- è½»é‡ EmbeddingDropout é’©å­ ----------\n",
    "class EmbeddingDropout(nn.Module):\n",
    "    def __init__(self, p: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.p = float(p)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.training or self.p <= 0.0:\n",
    "            return x\n",
    "        # x: (B, F, D)ï¼Œé€å…ƒç´  dropout\n",
    "        return F.dropout(x, p=self.p, training=True)\n",
    "\n",
    "class RealCIN(nn.Module):\n",
    "    \"\"\"\n",
    "    çœŸå®çš„ CINï¼ˆCompressed Interaction Networkï¼‰\n",
    "    è¾“å…¥: x å½¢çŠ¶ [B, F, D]\n",
    "    è¿‡ç¨‹: X_{k-1} ä¸ X_0 å¤–ç§¯ -> 1x1 Conv èšåˆ -> (B, H_k, D)\n",
    "    è¾“å‡º: æ‹¼æ¥æ¯å±‚å¯¹ D æ±‚å’Œåçš„ (B, sum(H_k))\n",
    "    \"\"\"\n",
    "    def __init__(self, field_num: int, embed_dim: int, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.field_nums = [field_num]\n",
    "        self.embed_dim = embed_dim\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList()  # âœ¨ æ¯å±‚åš BN æŠ‘åˆ¶é€šé“å°ºåº¦æ¼‚ç§»\n",
    "        for size in layer_sizes:\n",
    "            in_channels = self.field_nums[-1] * self.field_nums[0]\n",
    "            self.convs.append(nn.Conv1d(in_channels=in_channels,\n",
    "                                        out_channels=size, kernel_size=1, bias=False))\n",
    "            self.bns.append(nn.BatchNorm1d(size))\n",
    "            self.field_nums.append(size)\n",
    "\n",
    "    def forward(self, x):  # x: (B, F0, D)\n",
    "        x0 = x\n",
    "        xs = [x]\n",
    "        outputs = []\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            # å¤–ç§¯: (B, F_{k-1}, F0, D)\n",
    "            z = torch.einsum('bfd,bhd->bfhd', xs[-1], x0)\n",
    "            B, Fk_1, F0, D = z.size()\n",
    "            z = z.reshape(B, Fk_1 * F0, D).contiguous()  # (B, C_in, L=D)\n",
    "            z = conv(z)                                  # (B, H_k, D)\n",
    "            z = bn(z)\n",
    "            z = F.gelu(z)\n",
    "            outputs.append(z.sum(dim=2))                 # (B, H_k)\n",
    "            xs.append(z)                                 # (B, H_k, D)\n",
    "        return torch.cat(outputs, dim=1) if outputs else torch.zeros(x.size(0), 0, device=x.device)\n",
    "\n",
    "class XDeepFM(nn.Module):\n",
    "    \"\"\"\n",
    "    xDeepFMï¼ˆCIN + DNNï¼‰ï¼š\n",
    "    - user_idx / asset_idx å„ä¸€ä¸ª Embeddingï¼ˆIDåŸŸï¼‰\n",
    "    - ç”¨æˆ·/èµ„äº§ç±»åˆ«ç‰¹å¾ï¼šæ¯åˆ—ä¸€ä¸ª Embedding\n",
    "    - æ•°å€¼ç‰¹å¾ï¼šæ¯åˆ—ä¸€ä¸ª Linear(1->D)ï¼Œä½œä¸ºç‹¬ç«‹ field\n",
    "    - CIN æ˜¾å¼äº¤äº’ + DNN éšå¼äº¤äº’ -> æ‹¼æ¥è¾“å‡º logits\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        user_num: int,\n",
    "        asset_num: int,\n",
    "        embed_dim: int,\n",
    "        user_feats_sizes: list,   # æ¯ä¸ªç”¨æˆ·ç±»ç›®çš„ vocab size\n",
    "        asset_feats_sizes: list,  # æ¯ä¸ªèµ„äº§ç±»ç›®çš„ vocab size\n",
    "        asset_num_feat_dim: int,  # æ•°å€¼ç‰¹å¾åˆ—æ•°\n",
    "        cin_sizes = CIN_SIZES,\n",
    "        dnn_hidden = DNN_HIDDEN,\n",
    "        dropout = DROPOUT,\n",
    "        emb_dropout: float = EMB_DROPOUT_DEFAULT,\n",
    "        base_pos_rate: float | None = None,  # ç”¨æ­£ä¾‹åŸºå‡†ç‡åˆå§‹åŒ–è¾“å‡º bias\n",
    "        logit_sign: int = 1,                 # âœ¨ ææ€§å¼€å…³ï¼š-1 å¯æ•´ä½“å–å\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim  = embed_dim\n",
    "        self.logit_sign = 1 if logit_sign >= 0 else -1\n",
    "\n",
    "        # ID Embeddings\n",
    "        self.user_emb  = nn.Embedding(user_num, embed_dim)\n",
    "        self.asset_emb = nn.Embedding(asset_num, embed_dim)\n",
    "\n",
    "        # Categorical Embeddings\n",
    "        self.user_cat_embs  = nn.ModuleList([nn.Embedding(v, embed_dim) for v in user_feats_sizes])\n",
    "        self.asset_cat_embs = nn.ModuleList([nn.Embedding(v, embed_dim) for v in asset_feats_sizes])\n",
    "\n",
    "        # Numeric -> Embedding\n",
    "        self.asset_num_projs = nn.ModuleList([nn.Linear(1, embed_dim) for _ in range(asset_num_feat_dim)])\n",
    "\n",
    "        # field æ€»æ•°\n",
    "        self.field_num = 2 + len(user_feats_sizes) + len(asset_feats_sizes) + asset_num_feat_dim\n",
    "\n",
    "        # CINï¼ˆæ˜¾å¼äº¤äº’ï¼‰\n",
    "        self.cin = RealCIN(field_num=self.field_num, embed_dim=embed_dim, layer_sizes=cin_sizes)\n",
    "        cin_out_dim = sum(cin_sizes)\n",
    "\n",
    "        # DNNï¼ˆéšå¼äº¤äº’ï¼‰ï¼šLinear + LayerNorm + GELU + Dropout\n",
    "        dnn_in_dim = self.field_num * embed_dim\n",
    "        dnn_layers = []\n",
    "        in_dim = dnn_in_dim\n",
    "        for h in dnn_hidden:\n",
    "            dnn_layers += [nn.Linear(in_dim, h), nn.LayerNorm(h), nn.GELU(), nn.Dropout(dropout)]\n",
    "            in_dim = h\n",
    "        self.dnn = nn.Sequential(*dnn_layers)\n",
    "\n",
    "        # Embedding Dropout\n",
    "        self.emb_dropout = EmbeddingDropout(p=emb_dropout)\n",
    "\n",
    "        # è¾“å‡ºå±‚\n",
    "        self.output = nn.Linear(cin_out_dim + dnn_hidden[-1], 1)\n",
    "\n",
    "        self._reset_params(base_pos_rate)\n",
    "\n",
    "    def _reset_params(self, base_pos_rate: float | None):\n",
    "        nn.init.xavier_uniform_(self.user_emb.weight)\n",
    "        nn.init.xavier_uniform_(self.asset_emb.weight)\n",
    "        for emb in self.user_cat_embs:  nn.init.xavier_uniform_(emb.weight)\n",
    "        for emb in self.asset_cat_embs: nn.init.xavier_uniform_(emb.weight)\n",
    "        for proj in self.asset_num_projs:\n",
    "            nn.init.xavier_uniform_(proj.weight); nn.init.zeros_(proj.bias)\n",
    "        for m in self.dnn:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        if base_pos_rate is not None and 0.0 < base_pos_rate < 1.0:\n",
    "            with torch.no_grad():\n",
    "                bias = math.log(base_pos_rate / (1.0 - base_pos_rate))\n",
    "                self.output.bias.fill_(bias)   # å†·å¯åŠ¨æ›´ç¨³\n",
    "        else:\n",
    "            nn.init.zeros_(self.output.bias)\n",
    "\n",
    "    def forward(self, user_idx, user_cat_feats, asset_idx, asset_cat_feats, asset_num_feats):\n",
    "\n",
    "        B = user_idx.size(0)\n",
    "\n",
    "        # ID åŸŸ\n",
    "        user_e  = self.user_emb(user_idx).unsqueeze(1)    # (B, 1, D)\n",
    "        asset_e = self.asset_emb(asset_idx).unsqueeze(1)  # (B, 1, D)\n",
    "\n",
    "        # ç±»åˆ«åŸŸ\n",
    "        user_cat_list  = [emb(user_cat_feats[:, i]).unsqueeze(1)  for i, emb in enumerate(self.user_cat_embs)]\n",
    "        asset_cat_list = [emb(asset_cat_feats[:, i]).unsqueeze(1) for i, emb in enumerate(self.asset_cat_embs)]\n",
    "\n",
    "        # æ•°å€¼åŸŸ\n",
    "        asset_num_feats = asset_num_feats.contiguous()\n",
    "        asset_num_list = [proj(asset_num_feats[:, i:i+1]).unsqueeze(1) for i, proj in enumerate(self.asset_num_projs)]\n",
    "\n",
    "        # æ‹¼æ¥ + EmbeddingDropout\n",
    "        x = torch.cat([user_e, asset_e] + user_cat_list + asset_cat_list + asset_num_list, dim=1)  # (B, F, D)\n",
    "        x = self.emb_dropout(x).contiguous()\n",
    "\n",
    "        # CIN æ˜¾å¼äº¤äº’\n",
    "        cin_out = self.cin(x)  # (B, sum(CIN_SIZES))\n",
    "\n",
    "        # DNN éšå¼äº¤äº’\n",
    "        x_flat = x.reshape(B, -1).contiguous()\n",
    "        dnn_out = self.dnn(x_flat) # (B, dnn_hidden[-1])\n",
    "\n",
    "        # æ‹¼æ¥è¾“å‡º\n",
    "        logits = self.output(torch.cat([cin_out, dnn_out], dim=1)).squeeze(1)  # (B,)\n",
    "        return self.logit_sign * logits\n",
    "\n",
    "# -------------------------------\n",
    "# æ„å»ºæ¨¡å‹å®ä¾‹\n",
    "# -------------------------------\n",
    "\n",
    "def _get_vocab_sizes(df: pd.DataFrame, cols: list) -> list:\n",
    "    # vocab size = max(index) + 1ï¼›LabelEncoder ç”Ÿæˆçš„æ˜¯ 0..K-1\n",
    "    return [int(df[c].max()) + 1 for c in cols]\n",
    "\n",
    "user_feats_sizes = _get_vocab_sizes(customers_latest, user_cat_features)\n",
    "asset_feats_sizes = _get_vocab_sizes(assets_latest, asset_cat_features)\n",
    "\n",
    "user_num = len(user2idx)\n",
    "asset_num = len(asset2idx)\n",
    "asset_num_feat_dim = len(num_cols)\n",
    "\n",
    "model = XDeepFM(\n",
    "    user_num=user_num,\n",
    "    asset_num=asset_num,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    user_feats_sizes=user_feats_sizes,\n",
    "    asset_feats_sizes=asset_feats_sizes,\n",
    "    asset_num_feat_dim=asset_num_feat_dim,\n",
    "    cin_sizes=CIN_SIZES,\n",
    "    dnn_hidden=DNN_HIDDEN,\n",
    "    dropout=DROPOUT,\n",
    "    emb_dropout=EMB_DROPOUT_DEFAULT,\n",
    "    base_pos_rate=None,  # è‹¥çŸ¥é“æ­£ä¾‹æ¯”ä¾‹ pï¼Œå¡« pï¼ˆå¦‚ 0.12ï¼‰\n",
    "    logit_sign=1,        # è‹¥â€œflipped AUCâ€æ›´é«˜ï¼Œæ”¹ä¸º -1\n",
    ").to(DEVICE)\n",
    "\n",
    "# ---------- å‚æ•°åˆ†ç»„ helperï¼šembedding vs others ----------\n",
    "def get_param_groups_for_adamw(model: nn.Module):\n",
    "    \"\"\"\n",
    "    è¿”å›ä¸¤ä¸ª param ç»„ï¼Œä¾¿äºè®­ç»ƒæ—¶ï¼š\n",
    "      - å¯¹ embedding ç±»å‚æ•°è®¾ weight_decay=0.0\n",
    "      - å…¶å®ƒå±‚ç”¨å…¨å±€ weight_decay\n",
    "    \"\"\"\n",
    "    emb_params, other_params = [], []\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if ('emb' in name) or ('Embedding' in p.__class__.__name__):\n",
    "            emb_params.append(p)\n",
    "        else:\n",
    "            other_params.append(p)\n",
    "    return emb_params, other_params\n",
    "\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"==== xDeepFM æ¨¡å‹æ„å»ºå®Œæˆ ====\")\n",
    "print(f\"è®¾å¤‡: {DEVICE}\")\n",
    "print(f\"Embed ç»´åº¦: {EMBED_DIM}\")\n",
    "print(f\"Field æ•°é‡: 2(ID) + {len(user_cat_features)}(UserCat) + {len(asset_cat_features)}(AssetCat) + {len(num_cols)}(AssetNum) = {model.field_num}\")\n",
    "print(f\"CIN å±‚: {CIN_SIZES} -> è¾“å‡ºç»´åº¦ {sum(CIN_SIZES)}\")\n",
    "print(f\"DNN ç»“æ„: {DNN_HIDDEN}\")\n",
    "print(f\"å¯è®­ç»ƒå‚æ•°é‡: {count_params(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eaa054b-d1b4-4b48-981b-251f04a58c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cuda; AMP=True\n",
      "âœ… torch.compile enabled (reduce-overhead)\n",
      "[DataLoader] multiprocess ON: workers=8, persistent=False, prefetch=1, start_method=fork\n",
      "[ClassStats] pos=1044, neg=4076, pos_weightâ‰ˆ3.90\n",
      "å¼€å§‹è®­ç»ƒï¼šEPOCHS=6, BATCH_SIZE=512, workers=8, pin=True, persistent=False\n",
      "\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] TrainLoss=0.94924 | ValidLoss=0.86738 | AUC=0.88997 | LogLoss=0.51499\n",
      "ğŸŸ¢ New best (auc): save -> xdeepfm_best.pt\n",
      "\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 02] TrainLoss=0.86183 | ValidLoss=0.85455 | AUC=0.89877 | LogLoss=0.50571\n",
      "ğŸŸ¢ New best (auc): save -> xdeepfm_best.pt\n",
      "\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 03] TrainLoss=0.80915 | ValidLoss=0.85728 | AUC=0.89672 | LogLoss=0.48661\n",
      "\n",
      "Epoch 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 04] TrainLoss=0.76291 | ValidLoss=0.87533 | AUC=0.88876 | LogLoss=0.46923\n",
      "\n",
      "Epoch 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 05] TrainLoss=0.72335 | ValidLoss=0.89767 | AUC=0.88157 | LogLoss=0.49263\n",
      "\n",
      "Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 06] TrainLoss=0.70953 | ValidLoss=0.90440 | AUC=0.87766 | LogLoss=0.48301\n",
      "\n",
      "âœ… è®­ç»ƒå®Œæˆï¼›æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ xdeepfm_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 3: è®­ç»ƒå¾ªç¯ï¼ˆå¯å¤šè¿›ç¨‹åŠ é€Ÿï¼‰ã€ç¨³å¥åŠ é€Ÿç‰ˆã€‘\n",
    "# - AMP + TF32 + torch.compile\n",
    "# - DataLoader åœ¨ Notebook/Linux ä¼˜å…ˆä½¿ç”¨ forkï¼Œå°‘é‡ worker æ›´ç¨³\n",
    "# - Windows è‡ªåŠ¨é™çº§å•è¿›ç¨‹\n",
    "# ================================\n",
    "import os, gc, sys, math, platform, numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "EPOCHS = 6\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "GRAD_CLIP_NORM = 5.0\n",
    "BEST_PATH = \"xdeepfm_best.pt\"\n",
    "\n",
    "MONITOR = \"auc\"; MODE = \"max\"\n",
    "EARLY_STOP = True\n",
    "PATIENCE = 2\n",
    "MIN_DELTA = 5e-4\n",
    "\n",
    "USE_SCHED = True\n",
    "LR_MIN = 1e-5\n",
    "SCHED_CFG = dict(\n",
    "    mode='max', factor=0.5, patience=1, cooldown=1,\n",
    "    threshold=1e-4, threshold_mode='abs', min_lr=LR_MIN\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "BF16_OK = (USE_AMP and torch.cuda.is_bf16_supported())\n",
    "print(f\"[Device] {DEVICE}; AMP={USE_AMP}\")\n",
    "\n",
    "# â€”â€” æ•°å€¼åŠ é€Ÿï¼šTF32 / compile â€”â€”\n",
    "try:\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if USE_AMP:\n",
    "    try:\n",
    "        # åœ¨åŸé…ç½®ä¸Šä¿æŒ reduce-overheadï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰\n",
    "        model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "        print(\"âœ… torch.compile enabled (reduce-overhead)\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ torch.compile failed:\", e)\n",
    "\n",
    "# â€”â€” ä¸€é”®å¼€å…³ï¼šå¤šè¿›ç¨‹ DataLoader â€”â€”\n",
    "MULTIWORKER = True          # å¯æŒ‰éœ€å…³é—­\n",
    "FORCE_MULTIPROC = True      \n",
    "def _in_notebook(): return ('ipykernel' in sys.modules) or ('IPython' in sys.modules)\n",
    "IS_WINDOWS = (platform.system() == \"Windows\")\n",
    "\n",
    "# é»˜è®¤æ‰¹å¤§å°ï¼šGPU ç¨å¤§ã€CPU ä¿å®ˆ\n",
    "BATCH_SIZE = 512 if USE_AMP else 128\n",
    "\n",
    "if MULTIWORKER and (FORCE_MULTIPROC or (not _in_notebook())) and (not IS_WINDOWS):\n",
    "    # âœ… åœ¨ Linux/WSL ä¸Šä¼˜å…ˆä½¿ç”¨ forkï¼Œèƒ½è§„é¿ notebook + spawn çš„ pickling é—®é¢˜\n",
    "    try:\n",
    "        import multiprocessing as mp\n",
    "        mp.set_start_method(\"fork\", force=True)\n",
    "        start_method = \"fork\"\n",
    "    except Exception as e:\n",
    "        print(\"[DataLoader] set_start_method('fork') å¤±è´¥ï¼Œå›é€€ spawnï¼š\", e)\n",
    "        try:\n",
    "            import multiprocessing as mp\n",
    "            mp.set_start_method(\"spawn\", force=True)\n",
    "            start_method = \"spawn\"\n",
    "        except Exception as e2:\n",
    "            print(\"[DataLoader] set_start_method('spawn') ä¹Ÿå¤±è´¥ï¼Œé€€å›å•è¿›ç¨‹ï¼š\", e2)\n",
    "            NUM_WORKERS = 0\n",
    "            PIN_MEMORY  = USE_AMP\n",
    "            PERSISTENT  = False\n",
    "            PREFETCH    = None\n",
    "            start_method = \"none\"\n",
    "    # ä»…å½“æˆåŠŸè®¾ç½®äº†å¯åŠ¨æ–¹å¼æ‰é…ç½®å¤šè¿›ç¨‹å‚æ•°\n",
    "    if 'start_method' in locals() and start_method in (\"fork\", \"spawn\"):\n",
    "        NUM_WORKERS = 8          # ä¸ç”¨å¤ªå¤šï¼Œç¨³ä¸ºä¸»\n",
    "        PIN_MEMORY  = USE_AMP\n",
    "        PERSISTENT  = False      # Notebook ä¸‹å»ºè®®å…ˆå…³\n",
    "        PREFETCH    = 1\n",
    "        print(f\"[DataLoader] multiprocess ON: workers={NUM_WORKERS}, \"\n",
    "              f\"persistent={PERSISTENT}, prefetch={PREFETCH}, start_method={start_method}\")\n",
    "else:\n",
    "    # å•è¿›ç¨‹æ›´ç¨³ï¼ˆWindows / æ˜¾å¼å…³é—­ / Notebook æœªå¼ºåˆ¶ï¼‰\n",
    "    NUM_WORKERS = 0\n",
    "    PIN_MEMORY  = USE_AMP\n",
    "    PERSISTENT  = False\n",
    "    PREFETCH    = None\n",
    "    if MULTIWORKER and _in_notebook() and not FORCE_MULTIPROC:\n",
    "        print(\"[DataLoader] Notebook ç¯å¢ƒï¼šè‡ªåŠ¨é™çº§ workers=0ï¼ˆæ›´ç¨³ï¼‰\")\n",
    "    elif MULTIWORKER and IS_WINDOWS:\n",
    "        print(\"[DataLoader] Windows ç¯å¢ƒï¼šè‡ªåŠ¨é™çº§ workers=0\")\n",
    "\n",
    "def _build_loader(ds, shuffle):\n",
    "    return DataLoader(\n",
    "        ds, batch_size=BATCH_SIZE, shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=(PERSISTENT if NUM_WORKERS>0 else False),\n",
    "        prefetch_factor=(PREFETCH if (PREFETCH and NUM_WORKERS>0) else None)\n",
    "    )\n",
    "\n",
    "# ç›´æ¥å¤ç”¨ Step 1 äº§å‡ºçš„ train_dataset / valid_dataset\n",
    "train_loader = _build_loader(train_dataset, True)\n",
    "valid_loader = _build_loader(valid_dataset, False)\n",
    "\n",
    "# â€”â€” ä¼˜åŒ–å™¨/è°ƒåº¦å™¨/AMP â€”â€”\n",
    "def _group_adamw(m, lr, wd, fused):\n",
    "    emb_ids = set()\n",
    "    for mod in m.modules():\n",
    "        if isinstance(mod, nn.Embedding):\n",
    "            for p in mod.parameters(recurse=False):\n",
    "                emb_ids.add(id(p))\n",
    "    emb, other = [], []\n",
    "    for p in m.parameters():\n",
    "        if not p.requires_grad: continue\n",
    "        (emb if id(p) in emb_ids else other).append(p)\n",
    "    try:\n",
    "        opt = torch.optim.AdamW(\n",
    "            [{\"params\": emb, \"weight_decay\": 0.0},\n",
    "             {\"params\": other, \"weight_decay\": wd}],\n",
    "            lr=lr, fused=fused)\n",
    "    except TypeError:\n",
    "        opt = torch.optim.AdamW(\n",
    "            [{\"params\": emb, \"weight_decay\": 0.0},\n",
    "             {\"params\": other, \"weight_decay\": wd}],\n",
    "            lr=lr)\n",
    "    return opt\n",
    "\n",
    "optimizer = _group_adamw(model, LR, WEIGHT_DECAY, fused=USE_AMP)\n",
    "scheduler = ReduceLROnPlateau(optimizer, **SCHED_CFG) if USE_SCHED else None\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "# â€”â€” ç±»åˆ«ä¸å¹³è¡¡ä¼°è®¡ â€”â€”\n",
    "@torch.no_grad()\n",
    "def estimate_pos_weight(loader, sample_batches: int = 10):\n",
    "    pos = neg = checked = 0\n",
    "    for batch in loader:\n",
    "        y = batch[-1]\n",
    "        y = torch.nan_to_num(y, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "        pos += int((y > 0.5).sum().item())\n",
    "        neg += int((y <= 0.5).sum().item())\n",
    "        checked += 1\n",
    "        if checked >= sample_batches: break\n",
    "    pos = max(pos, 1); neg = max(neg, 1)\n",
    "    pw = neg / pos\n",
    "    print(f\"[ClassStats] pos={pos}, neg={neg}, pos_weightâ‰ˆ{pw:.2f}\")\n",
    "    return torch.tensor([pw], device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "POS_WEIGHT   = estimate_pos_weight(train_loader, sample_batches=10)\n",
    "LABEL_SMOOTH = 0.05\n",
    "\n",
    "def _to_device(batch):\n",
    "    nb = (DEVICE.type == 'cuda')\n",
    "    return tuple(t.to(DEVICE, non_blocking=nb) for t in batch)\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    tot_loss = tot_samples = 0\n",
    "    skipped = 0\n",
    "    pbar = tqdm(train_loader, desc=\"Train\", dynamic_ncols=True, mininterval=0.3, leave=False)\n",
    "    for batch in pbar:\n",
    "        batch = _to_device(batch)\n",
    "        if batch[-1].numel() == 0:\n",
    "            skipped += 1; continue\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type=DEVICE.type, enabled=USE_AMP,\n",
    "                                dtype=torch.bfloat16 if BF16_OK else torch.float16):\n",
    "            logits = torch.clamp(model(*batch[:-1]), -20, 20)\n",
    "            y = torch.nan_to_num(batch[-1], nan=0.0, posinf=1.0, neginf=0.0)\n",
    "            targets = y * (1.0 - LABEL_SMOOTH) + (1.0 - y) * LABEL_SMOOTH\n",
    "            per_sample = F.binary_cross_entropy_with_logits(\n",
    "                logits, targets, reduction='none', pos_weight=POS_WEIGHT)\n",
    "        mask = torch.isfinite(per_sample)\n",
    "        if not mask.any():\n",
    "            skipped += 1; continue\n",
    "        loss = per_sample[mask].mean()\n",
    "        scaler.scale(loss).backward()\n",
    "        if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        tot_loss   += loss.item() * int(mask.sum().item())\n",
    "        tot_samples += int(mask.sum().item())\n",
    "        pbar.set_postfix(loss=f\"{(tot_loss/max(1,tot_samples)):.5f}\")\n",
    "    pbar.close()\n",
    "    if skipped: print(f\"[WARN] train skipped {skipped} batches\")\n",
    "    return tot_loss / max(1, tot_samples)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    tot_loss = tot_samples = 0\n",
    "    skipped = 0\n",
    "    all_logits, all_labels = [], []\n",
    "    pbar = tqdm(valid_loader, desc=\"Valid\", dynamic_ncols=True, mininterval=0.5, leave=False)\n",
    "    with torch.amp.autocast(device_type=DEVICE.type, enabled=USE_AMP,\n",
    "                            dtype=torch.bfloat16 if BF16_OK else torch.float16):\n",
    "        for batch in pbar:\n",
    "            batch = _to_device(batch)\n",
    "            if batch[-1].numel() == 0:\n",
    "                skipped += 1; continue\n",
    "            logits = torch.clamp(model(*batch[:-1]), -20, 20)\n",
    "            y = torch.nan_to_num(batch[-1], nan=0.0, posinf=1.0, neginf=0.0)\n",
    "            targets = y * (1.0 - LABEL_SMOOTH) + (1.0 - y) * LABEL_SMOOTH\n",
    "            per_sample = F.binary_cross_entropy_with_logits(\n",
    "                logits, targets, reduction='none', pos_weight=POS_WEIGHT)\n",
    "            mask = torch.isfinite(per_sample)\n",
    "            if not mask.any():\n",
    "                skipped += 1; continue\n",
    "            loss = per_sample[mask].mean()\n",
    "            tot_loss += loss.item() * int(mask.sum().item())\n",
    "            tot_samples += int(mask.sum().item())\n",
    "            all_logits.append(logits[mask].detach().cpu().float().numpy())\n",
    "            all_labels.append(y[mask].detach().cpu().numpy())\n",
    "    pbar.close()\n",
    "    if skipped: print(f\"[WARN] valid skipped {skipped} batches\")\n",
    "    if tot_samples == 0: return math.nan, math.nan, math.nan\n",
    "    logits = np.concatenate(all_logits); labels = np.concatenate(all_labels).astype(np.int64)\n",
    "    probs = np.clip(1.0/(1.0+np.exp(-logits)), 1e-7, 1-1e-7)\n",
    "    try: auc = roc_auc_score(labels, probs)\n",
    "    except Exception: auc = math.nan\n",
    "    try: ll = log_loss(labels, probs)\n",
    "    except Exception: ll = math.nan\n",
    "    return (tot_loss / tot_samples), auc, ll\n",
    "\n",
    "def _is_better(curr, best):\n",
    "    if best is None: return True\n",
    "    delta = curr - best if MODE == \"max\" else best - curr\n",
    "    return delta > MIN_DELTA\n",
    "\n",
    "history = {\"train_loss\": [], \"valid_loss\": [], \"valid_auc\": [], \"valid_logloss\": []}\n",
    "best_metric = None\n",
    "print(f\"å¼€å§‹è®­ç»ƒï¼šEPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, workers={NUM_WORKERS}, pin={PIN_MEMORY}, persistent={(PERSISTENT if NUM_WORKERS>0 else False)}\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    tr = train_one_epoch()\n",
    "    vl, va, ll = evaluate()\n",
    "    history[\"train_loss\"].append(tr)\n",
    "    history[\"valid_loss\"].append(vl)\n",
    "    history[\"valid_auc\"].append(va)\n",
    "    history[\"valid_logloss\"].append(ll)\n",
    "    mon = va if MONITOR==\"auc\" else (-ll)\n",
    "    print(f\"[Epoch {epoch:02d}] TrainLoss={tr:.5f} | ValidLoss={vl:.5f} | AUC={va:.5f} | LogLoss={ll:.5f}\")\n",
    "\n",
    "    if _is_better(mon, best_metric):\n",
    "        best_metric = mon\n",
    "        torch.save(model.state_dict(), BEST_PATH)\n",
    "        print(f\"ğŸŸ¢ New best ({MONITOR}): save -> {BEST_PATH}\")\n",
    "\n",
    "    if USE_SCHED and scheduler is not None:\n",
    "        scheduler.step(va if not math.isnan(va) else -1.0)\n",
    "        cur_lr = min(g['lr'] for g in optimizer.param_groups)\n",
    "        if cur_lr <= LR_MIN and not _is_better(mon, best_metric):\n",
    "            print(f\"â¹ LR å·²åˆ°ä¸‹é™({LR_MIN:.1e})ä¸”æŒ‡æ ‡æ— æå‡ï¼Œæå‰åœæ­¢ã€‚\")\n",
    "            break\n",
    "\n",
    "    if EARLY_STOP and epoch >= PATIENCE:\n",
    "        recent = history[\"valid_auc\"][-PATIENCE:] if MONITOR==\"auc\" else [-x for x in history[\"valid_logloss\"][-PATIENCE:]]\n",
    "        if len(recent)==PATIENCE and max(recent)-min(recent) < MIN_DELTA:\n",
    "            print(f\"â¹ Early stop triggered at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "if best_metric is None and not os.path.exists(BEST_PATH):\n",
    "    torch.save(model.state_dict(), BEST_PATH)\n",
    "print(f\"\\nâœ… è®­ç»ƒå®Œæˆï¼›æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ {BEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4249c9f-39db-4d23-afbb-da29eaf38fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step3B] å¼€å§‹è¯„ä¼°ä¸ç»˜å›¾ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰\n",
      "[Step3B] å·²åŠ è½½æœ€ä½³æƒé‡ï¼šxdeepfm_best.pt\n",
      "[Step3B] æ¨ç†å®Œæˆï¼š89884 æ ·æœ¬ï¼Œç”¨æ—¶ 12.74sï¼›batch=2048, workers=8\n",
      "[Step3B] æŒ‡æ ‡ï¼šAUC=0.8988 | AP=0.7229 | Logloss=0.50573 | KS=0.0000 | Brier=0.16279\n",
      "[Step3B] åˆ†ç±»@thr=0.5ï¼š P_macro=0.7180 | R_macro=0.8155 | F1_macro=0.7325 || P_micro=0.7833 | R_micro=0.7833 | F1_micro=0.7833\n",
      "[Step3B] åˆ†ç±»@thr*=BestF1ï¼š thr*=0.6758 | P_macro=0.7705 | R_macro=0.8067 | F1_macro=0.7858 || P_micro=0.8534 | R_micro=0.8534 | F1_micro=0.8534\n",
      "[Step3B] âœ… å·²ä¿å­˜ï¼šxdeepfm_cls_report.csv\n",
      "\n",
      "[ç ”ç©¶ç‰ˆ] åˆ†ç»„è¯„ä¼°åˆ†æï¼š\n",
      "  Warmç”¨æˆ· (n=89854): AUC=0.8988 | AP=0.7230 | Brier=0.16276\n",
      "    åˆ†ç±»@0.5: F1_macro=0.7326 | F1_micro=0.7833 ; åˆ†ç±»@BestF1: thr*=0.6758 | F1_macro=0.7858 | F1_micro=0.8534\n",
      "  Coldç”¨æˆ· (n=30): AUC=0.7500 | AP=0.4710 | Brier=0.25496\n",
      "    åˆ†ç±»@0.5: F1_macro=0.5971 | F1_micro=0.6333 ; åˆ†ç±»@BestF1: thr*=0.6445 | F1_macro=0.6591 | F1_micro=0.7333\n",
      "  Warmèµ„äº§ (n=89864): AUC=0.8988 | AP=0.7230 | Brier=0.16277\n",
      "    åˆ†ç±»@0.5: F1_macro=0.7326 | F1_micro=0.7833 ; åˆ†ç±»@BestF1: thr*=0.6758 | F1_macro=0.7858 | F1_micro=0.8534\n",
      "  Coldèµ„äº§ (n=20): AUC=0.3056 | AP=0.1042 | Brier=0.24496\n",
      "    åˆ†ç±»@0.5: F1_macro=0.3548 | F1_micro=0.5500 ; åˆ†ç±»@BestF1: thr*=0.2754 | F1_macro=0.2481 | F1_micro=0.2500\n",
      "  çƒ­é—¨èµ„äº§Top10% (n=9074): AUC=0.7737 | AP=0.8821 | Brier=0.20210\n",
      "    åˆ†ç±»@0.5: F1_macro=0.4247 | F1_micro=0.7234 ; åˆ†ç±»@BestF1: thr*=0.7734 | F1_macro=0.6279 | F1_micro=0.7690\n",
      "  æ™®é€šèµ„äº§ (n=80790): AUC=0.8698 | AP=0.5701 | Brier=0.15836\n",
      "    åˆ†ç±»@0.5: F1_macro=0.6914 | F1_micro=0.7900 ; åˆ†ç±»@BestF1: thr*=0.6445 | F1_macro=0.7329 | F1_micro=0.8561\n",
      "  è®­ç»ƒé›†è§è¿‡çš„(user,asset)å¯¹ (n=10641): AUC=N/A | AP=N/A | Brier=0.18180\n",
      "    åˆ†ç±»@0.5: F1_macro=N/A | F1_micro=N/A ; åˆ†ç±»@BestF1: thr*=N/A | F1_macro=N/A | F1_micro=N/A\n",
      "  è®­ç»ƒé›†æœªè§çš„(user,asset)å¯¹ (n=79243): AUC=0.8983 | AP=0.7428 | Brier=0.16024\n",
      "    åˆ†ç±»@0.5: F1_macro=0.7491 | F1_micro=0.7879 ; åˆ†ç±»@BestF1: thr*=0.6758 | F1_macro=0.7913 | F1_micro=0.8469\n",
      "\n",
      "[Step3B] âœ… å·²ä¿å­˜ï¼šxdeepfm_ROC.png\n",
      "[Step3B] âœ… å·²ä¿å­˜ï¼šxdeepfm_PR.png\n",
      "[Step3B] ç¼“å­˜ï¼švalid_y_true.npy / valid_y_score.npy\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 3B: éªŒè¯é›†å¿«é€Ÿè¯„ä¼° + ROC/PR ç»˜å›¾ + åˆ†ç»„åˆ†æï¼ˆå¥å£®ç‰ˆï¼Œå«PRFï¼‰\n",
    "# ä¾èµ–ï¼šmodel, DEVICE, USE_AMP, BEST_PATH, valid_dataset, train_df\n",
    "# äº§ç‰©ï¼š\n",
    "#   - xdeepfm_ROC.png / xdeepfm_PR.png\n",
    "#   - valid_y_true.npy / valid_y_score.npy\n",
    "#   - xdeepfm_cls_report.csvï¼ˆåˆ†ç±»æŒ‡æ ‡æ±‡æ€»ï¼‰\n",
    "#   - Warm/Cold ç”¨æˆ·èµ„äº§åˆ†æã€çƒ­é—¨åº¦åˆ†ç»„è¯„ä¼°\n",
    "#   - å•ç±»åˆ†ç»„å¥å£®æ€§å›é€€\n",
    "#   - Precision/Recall/F1ï¼ˆmacro/microï¼‰@ å›ºå®šé˜ˆå€¼ä¸æœ€ä½³F1é˜ˆå€¼\n",
    "# ================================\n",
    "import os, time, platform, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score,\n",
    "    roc_auc_score, log_loss, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "print(\"\\n[Step3B] å¼€å§‹è¯„ä¼°ä¸ç»˜å›¾ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰\")\n",
    "\n",
    "# ---- åŠ è½½ best æƒé‡ï¼ˆè‹¥å­˜åœ¨ï¼‰----\n",
    "if os.path.exists(BEST_PATH):\n",
    "    state = torch.load(BEST_PATH, map_location=\"cpu\")\n",
    "    if isinstance(state, dict):\n",
    "        model.load_state_dict(state)\n",
    "        print(f\"[Step3B] å·²åŠ è½½æœ€ä½³æƒé‡ï¼š{BEST_PATH}\")\n",
    "    else:\n",
    "        print(f\"[Step3B] è¯»å– {BEST_PATH} ç»“æœé state_dictï¼Œè·³è¿‡åŠ è½½ã€‚\")\n",
    "else:\n",
    "    print(f\"[Step3B] æœªæ‰¾åˆ° {BEST_PATH}ï¼Œä½¿ç”¨å½“å‰å†…å­˜ä¸­çš„æ¨¡å‹å‚æ•°ã€‚\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# ---- Eval DataLoaderï¼ˆéªŒè¯é˜¶æ®µå¯ç”¨æ›´å¤§ batchï¼‰----\n",
    "IS_WINDOWS = (platform.system() == \"Windows\")\n",
    "CPU_COUNT = os.cpu_count() or 8\n",
    "EVAL_BATCH_SIZE = 2048 if USE_AMP else 512\n",
    "NUM_WORKERS = (max(2, CPU_COUNT // 2) if not IS_WINDOWS else 0) if USE_AMP else 0\n",
    "PIN_MEMORY  = bool(USE_AMP)\n",
    "PERSISTENT  = bool(USE_AMP and NUM_WORKERS > 0)\n",
    "PREFETCH    = 2 if (USE_AMP and NUM_WORKERS > 0) else None\n",
    "\n",
    "def make_eval_loader(dataset):\n",
    "    kwargs = dict(\n",
    "        dataset=dataset,\n",
    "        batch_size=EVAL_BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        persistent_workers=PERSISTENT\n",
    "    )\n",
    "    if PREFETCH is not None:\n",
    "        kwargs[\"prefetch_factor\"] = PREFETCH\n",
    "    return DataLoader(**kwargs)\n",
    "\n",
    "valid_loader_eval = make_eval_loader(valid_dataset)\n",
    "\n",
    "# ---- å°å·¥å…·ï¼šæ›´å¥å£®çš„æŒ‡æ ‡è®¡ç®— ----\n",
    "def _class_counts(y):\n",
    "    y = np.asarray(y).astype(int)\n",
    "    pos = int(np.sum(y == 1))\n",
    "    neg = int(np.sum(y == 0))\n",
    "    return pos, neg\n",
    "\n",
    "def quick_metrics(y_true, y_score, ks_bins=100):\n",
    "    \"\"\"\n",
    "    å¥å£®æŒ‡æ ‡ï¼š\n",
    "    * åªæœ‰ä¸¤ç±»åŒæ—¶å­˜åœ¨æ—¶æ‰è®¡ç®— AUC/AP/KSï¼›\n",
    "    * log_loss æ˜¾å¼ labels=[0,1]ï¼Œè‹¥åªæœ‰ä¸€ç±»è¿”å› np.nanï¼›\n",
    "    * Brier æ€»èƒ½ç®—ã€‚\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "    y_score = np.clip(y_score, 1e-7, 1-1e-7)\n",
    "\n",
    "    out = {}\n",
    "    pos, neg = _class_counts(y_true)\n",
    "    both = (pos > 0 and neg > 0)\n",
    "\n",
    "    if both:\n",
    "        out[\"auc\"] = float(roc_auc_score(y_true, y_score))\n",
    "        out[\"ap\"]  = float(average_precision_score(y_true, y_score))\n",
    "    else:\n",
    "        out[\"auc\"] = np.nan\n",
    "        out[\"ap\"]  = np.nan\n",
    "\n",
    "    try:\n",
    "        out[\"logloss\"] = float(log_loss(y_true, y_score, labels=[0, 1]))\n",
    "        if not both:\n",
    "            out[\"logloss\"] = np.nan\n",
    "    except Exception:\n",
    "        out[\"logloss\"] = np.nan\n",
    "\n",
    "    if both:\n",
    "        bins = np.linspace(0, 1, ks_bins+1)\n",
    "        idx = np.digitize(y_score, bins) - 1\n",
    "        tpr = np.zeros(ks_bins); fpr = np.zeros(ks_bins)\n",
    "        for b in range(ks_bins):\n",
    "            yb = y_true[idx == b]\n",
    "            if yb.size > 0:\n",
    "                tpr[b] = np.sum(yb == 1)\n",
    "                fpr[b] = np.sum(yb == 0)\n",
    "        tpr = np.cumsum(tpr) / max(pos, 1)\n",
    "        fpr = np.cumsum(fpr) / max(neg, 1)\n",
    "        out[\"ks\"] = float(np.max(tpr - fpr))\n",
    "    else:\n",
    "        out[\"ks\"] = np.nan\n",
    "\n",
    "    out[\"brier\"] = float(np.mean((y_score - y_true)**2))\n",
    "    return out\n",
    "\n",
    "def fmt(x):\n",
    "    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n",
    "        return \"N/A\"\n",
    "    return f\"{x:.4f}\"\n",
    "\n",
    "def prf_with_thresholds(y_true, y_score, fixed_thr=0.5):\n",
    "    \"\"\"\n",
    "    è¿”å›ä¸¤ç»„ PRFï¼šå›ºå®šé˜ˆå€¼ & æœ€ä¼˜F1é˜ˆå€¼ï¼ˆåœ¨éªŒè¯é›†ä¸Šç”± PR æ›²çº¿æœç´¢ï¼‰\n",
    "    è‹¥æ•°æ®åªæœ‰å•ä¸€ç±»åˆ«ï¼Œè¿”å› N/Aã€‚\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_score = np.asarray(y_score).astype(float)\n",
    "    pos, neg = _class_counts(y_true)\n",
    "    both = (pos > 0 and neg > 0)\n",
    "\n",
    "    def _val(v):\n",
    "        return np.nan if (v is None or (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))) else float(v)\n",
    "\n",
    "    out = {\n",
    "        \"fixed_thr\": fixed_thr,\n",
    "        \"fixed\": {\"prec_macro\": np.nan, \"rec_macro\": np.nan, \"f1_macro\": np.nan,\n",
    "                  \"prec_micro\": np.nan, \"rec_micro\": np.nan, \"f1_micro\": np.nan},\n",
    "        \"bestF1_thr\": np.nan,\n",
    "        \"bestF1\": {\"prec_macro\": np.nan, \"rec_macro\": np.nan, \"f1_macro\": np.nan,\n",
    "                   \"prec_micro\": np.nan, \"rec_micro\": np.nan, \"f1_micro\": np.nan},\n",
    "    }\n",
    "    if not both:\n",
    "        return out\n",
    "\n",
    "    # å›ºå®šé˜ˆå€¼\n",
    "    y_pred_fixed = (y_score >= fixed_thr).astype(int)\n",
    "    out[\"fixed\"][\"prec_macro\"] = _val(precision_score(y_true, y_pred_fixed, average='macro', zero_division=0))\n",
    "    out[\"fixed\"][\"rec_macro\"]  = _val(recall_score   (y_true, y_pred_fixed, average='macro', zero_division=0))\n",
    "    out[\"fixed\"][\"f1_macro\"]   = _val(f1_score       (y_true, y_pred_fixed, average='macro', zero_division=0))\n",
    "    out[\"fixed\"][\"prec_micro\"] = _val(precision_score(y_true, y_pred_fixed, average='micro', zero_division=0))\n",
    "    out[\"fixed\"][\"rec_micro\"]  = _val(recall_score   (y_true, y_pred_fixed, average='micro', zero_division=0))\n",
    "    out[\"fixed\"][\"f1_micro\"]   = _val(f1_score       (y_true, y_pred_fixed, average='micro', zero_division=0))\n",
    "\n",
    "    # æœ€ä¼˜F1é˜ˆå€¼ï¼ˆPRæ›²çº¿æœç´¢ï¼‰\n",
    "    prec, rec, thr = precision_recall_curve(y_true, y_score)\n",
    "    # f1 ä¸ thr å¯¹é½ï¼šå»æ‰ç¬¬ä¸€ä¸ªç‚¹ï¼ˆé€šå¸¸ rec=0, æ— é˜ˆå€¼ï¼‰\n",
    "    f1 = 2 * prec * rec / np.clip(prec + rec, 1e-12, None)\n",
    "    f1 = f1[1:]            # ä¸ thr å¯¹é½\n",
    "    if len(thr) > 0 and len(f1) == len(thr):\n",
    "        best = np.nanmax(f1)\n",
    "        cand = np.where(f1 == best)[0]\n",
    "        best_thr = float(np.median(thr[cand]))\n",
    "        out[\"bestF1_thr\"] = best_thr\n",
    "\n",
    "        y_pred_best = (y_score >= best_thr).astype(int)\n",
    "        out[\"bestF1\"][\"prec_macro\"] = _val(precision_score(y_true, y_pred_best, average='macro', zero_division=0))\n",
    "        out[\"bestF1\"][\"rec_macro\"]  = _val(recall_score   (y_true, y_pred_best, average='macro', zero_division=0))\n",
    "        out[\"bestF1\"][\"f1_macro\"]   = _val(f1_score       (y_true, y_pred_best, average='macro', zero_division=0))\n",
    "        out[\"bestF1\"][\"prec_micro\"] = _val(precision_score(y_true, y_pred_best, average='micro', zero_division=0))\n",
    "        out[\"bestF1\"][\"rec_micro\"]  = _val(recall_score   (y_true, y_pred_best, average='micro', zero_division=0))\n",
    "        out[\"bestF1\"][\"f1_micro\"]   = _val(f1_score       (y_true, y_pred_best, average='micro', zero_division=0))\n",
    "    return out\n",
    "\n",
    "def _fmt_or_na(v):\n",
    "    return \"N/A\" if (v is None or (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))) else f\"{v:.4f}\"\n",
    "\n",
    "# ---- æ¨ç†ï¼ˆAMP + inference_mode + é¢„åˆ†é…ï¼‰----\n",
    "N = len(valid_dataset)\n",
    "if N == 0:\n",
    "    print(\"[Step3B] âš ï¸ éªŒè¯é›†ä¸ºç©ºï¼Œè·³è¿‡ã€‚\")\n",
    "else:\n",
    "    y_true = np.empty(N, dtype=np.int64)\n",
    "    y_score = np.empty(N, dtype=np.float32)\n",
    "    # åˆ†ç»„åˆ†ææ‰€éœ€ç´¢å¼•\n",
    "    all_user_idx = np.empty(N, dtype=np.int64)\n",
    "    all_asset_idx = np.empty(N, dtype=np.int64)\n",
    "    offset = 0\n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.inference_mode():\n",
    "        for batch in valid_loader_eval:\n",
    "            # å‡è®¾æ•°æ®é›†è¿”å›ï¼šuser_idx, user_cat, asset_idx, asset_cat, asset_num, label\n",
    "            user_idx, user_cat, asset_idx, asset_cat, asset_num, label = batch\n",
    "\n",
    "            bs = label.size(0)\n",
    "            # ä¿å­˜ç´¢å¼•åˆ°CPUæ•°ç»„\n",
    "            if user_idx.device.type != 'cpu':\n",
    "                all_user_idx[offset:offset+bs] = user_idx.cpu().numpy()\n",
    "            else:\n",
    "                all_user_idx[offset:offset+bs] = user_idx.numpy()\n",
    "\n",
    "            if asset_idx.device.type != 'cpu':\n",
    "                all_asset_idx[offset:offset+bs] = asset_idx.cpu().numpy()\n",
    "            else:\n",
    "                all_asset_idx[offset:offset+bs] = asset_idx.numpy()\n",
    "\n",
    "            # æ¬åˆ°è®¾å¤‡\n",
    "            nb = (DEVICE.type == \"cuda\")\n",
    "            user_idx  = user_idx.to(DEVICE, non_blocking=nb)\n",
    "            user_cat  = user_cat.to(DEVICE, non_blocking=nb)\n",
    "            asset_idx = asset_idx.to(DEVICE, non_blocking=nb)\n",
    "            asset_cat = asset_cat.to(DEVICE, non_blocking=nb)\n",
    "            asset_num = asset_num.to(DEVICE, non_blocking=nb)\n",
    "\n",
    "            # å‰å‘\n",
    "            try:\n",
    "                # torch 2.x\n",
    "                with torch.amp.autocast(device_type=DEVICE.type, enabled=USE_AMP):\n",
    "                    logits = model(user_idx, user_cat, asset_idx, asset_cat, asset_num)\n",
    "                    probs  = torch.sigmoid(logits).float()\n",
    "            except Exception:\n",
    "                # å…¼å®¹ torch 1.x\n",
    "                with torch.cuda.amp.autocast(enabled=(USE_AMP and DEVICE.type == \"cuda\")):\n",
    "                    logits = model(user_idx, user_cat, asset_idx, asset_cat, asset_num)\n",
    "                    probs  = torch.sigmoid(logits).float()\n",
    "\n",
    "            # æ”¶é›†åˆ°CPU\n",
    "            l_cpu = label.detach().cpu().numpy()\n",
    "            y_true[offset:offset+bs]  = l_cpu.astype(np.int64)\n",
    "            y_score[offset:offset+bs] = probs.detach().cpu().numpy().astype(np.float32)\n",
    "            offset += bs\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[Step3B] æ¨ç†å®Œæˆï¼š{N} æ ·æœ¬ï¼Œç”¨æ—¶ {dt:.2f}sï¼›batch={EVAL_BATCH_SIZE}, workers={NUM_WORKERS}\")\n",
    "\n",
    "    # ---- æŒ‡æ ‡ï¼ˆå…¨é‡ï¼‰----\n",
    "    y_score = np.clip(y_score, 1e-7, 1 - 1e-7)\n",
    "    metrics = quick_metrics(y_true, y_score)\n",
    "    logloss_str = \"N/A\" if (metrics[\"logloss\"] is None or np.isnan(metrics[\"logloss\"])) else f\"{metrics['logloss']:.5f}\"\n",
    "    print(\n",
    "        \"[Step3B] æŒ‡æ ‡ï¼š\"\n",
    "        f\"AUC={fmt(metrics['auc'])} | AP={fmt(metrics['ap'])} | \"\n",
    "        f\"Logloss={logloss_str} | KS={fmt(metrics['ks'])} | Brier={metrics['brier']:.5f}\"\n",
    "    )\n",
    "\n",
    "    # ---- åŸºäºé˜ˆå€¼çš„åˆ†ç±»æŒ‡æ ‡ï¼ˆPrecision/Recall/F1ï¼šmacro & microï¼‰----\n",
    "    prf = prf_with_thresholds(y_true, y_score, fixed_thr=0.5)\n",
    "\n",
    "    print(\n",
    "        \"[Step3B] åˆ†ç±»@thr=0.5ï¼š \"\n",
    "        f\"P_macro={_fmt_or_na(prf['fixed']['prec_macro'])} | \"\n",
    "        f\"R_macro={_fmt_or_na(prf['fixed']['rec_macro'])} | \"\n",
    "        f\"F1_macro={_fmt_or_na(prf['fixed']['f1_macro'])} || \"\n",
    "        f\"P_micro={_fmt_or_na(prf['fixed']['prec_micro'])} | \"\n",
    "        f\"R_micro={_fmt_or_na(prf['fixed']['rec_micro'])} | \"\n",
    "        f\"F1_micro={_fmt_or_na(prf['fixed']['f1_micro'])}\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"[Step3B] åˆ†ç±»@thr*=BestF1ï¼š \"\n",
    "        f\"thr*={_fmt_or_na(prf['bestF1_thr'])} | \"\n",
    "        f\"P_macro={_fmt_or_na(prf['bestF1']['prec_macro'])} | \"\n",
    "        f\"R_macro={_fmt_or_na(prf['bestF1']['rec_macro'])} | \"\n",
    "        f\"F1_macro={_fmt_or_na(prf['bestF1']['f1_macro'])} || \"\n",
    "        f\"P_micro={_fmt_or_na(prf['bestF1']['prec_micro'])} | \"\n",
    "        f\"R_micro={_fmt_or_na(prf['bestF1']['rec_micro'])} | \"\n",
    "        f\"F1_micro={_fmt_or_na(prf['bestF1']['f1_micro'])}\"\n",
    "    )\n",
    "\n",
    "    # ä¿å­˜åˆ†ç±»æŠ¥å‘ŠCSV\n",
    "    try:\n",
    "        prf_row = {\n",
    "            \"thr_fixed\": prf[\"fixed_thr\"],\n",
    "            \"P_macro_fixed\": prf[\"fixed\"][\"prec_macro\"],\n",
    "            \"R_macro_fixed\": prf[\"fixed\"][\"rec_macro\"],\n",
    "            \"F1_macro_fixed\": prf[\"fixed\"][\"f1_macro\"],\n",
    "            \"P_micro_fixed\": prf[\"fixed\"][\"prec_micro\"],\n",
    "            \"R_micro_fixed\": prf[\"fixed\"][\"rec_micro\"],\n",
    "            \"F1_micro_fixed\": prf[\"fixed\"][\"f1_micro\"],\n",
    "            \"thr_bestF1\": prf[\"bestF1_thr\"],\n",
    "            \"P_macro_best\": prf[\"bestF1\"][\"prec_macro\"],\n",
    "            \"R_macro_best\": prf[\"bestF1\"][\"rec_macro\"],\n",
    "            \"F1_macro_best\": prf[\"bestF1\"][\"f1_macro\"],\n",
    "            \"P_micro_best\": prf[\"bestF1\"][\"prec_micro\"],\n",
    "            \"R_micro_best\": prf[\"bestF1\"][\"rec_micro\"],\n",
    "            \"F1_micro_best\": prf[\"bestF1\"][\"f1_micro\"],\n",
    "            \"AUC\": metrics[\"auc\"], \"AP\": metrics[\"ap\"], \"KS\": metrics[\"ks\"], \"Brier\": metrics[\"brier\"],\n",
    "            \"Logloss\": metrics[\"logloss\"],\n",
    "        }\n",
    "        pd.DataFrame([prf_row]).to_csv(\"xdeepfm_cls_report.csv\", index=False)\n",
    "        print(\"[Step3B] âœ… å·²ä¿å­˜ï¼šxdeepfm_cls_report.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Step3B] âš ï¸ ä¿å­˜åˆ†ç±»æŠ¥å‘Šå¤±è´¥ï¼š{e}\")\n",
    "\n",
    "    # ================================\n",
    "    # åˆ†ç»„è¯„ä¼°ï¼ˆè¯Šæ–­AUCè™šé«˜æ¥æºï¼‰\n",
    "    # ================================\n",
    "    print(\"\\n[ç ”ç©¶ç‰ˆ] åˆ†ç»„è¯„ä¼°åˆ†æï¼š\")\n",
    "\n",
    "    def print_group(name, mask):\n",
    "        n = int(mask.sum())\n",
    "        if n <= 0:\n",
    "            print(f\"  {name}ï¼šæ— æ ·æœ¬\")\n",
    "            return\n",
    "        m = quick_metrics(y_true[mask], y_score[mask])\n",
    "        print(\n",
    "            f\"  {name} (n={n}): \"\n",
    "            f\"AUC={fmt(m['auc'])} | AP={fmt(m['ap'])} | Brier={m['brier']:.5f}\"\n",
    "        )\n",
    "        # åˆ†ç»„F1\n",
    "        prf_g = prf_with_thresholds(y_true[mask], y_score[mask], fixed_thr=0.5)\n",
    "        print(\n",
    "            \"    åˆ†ç±»@0.5: \"\n",
    "            f\"F1_macro={_fmt_or_na(prf_g['fixed']['f1_macro'])} | \"\n",
    "            f\"F1_micro={_fmt_or_na(prf_g['fixed']['f1_micro'])} ; \"\n",
    "            \"åˆ†ç±»@BestF1: \"\n",
    "            f\"thr*={_fmt_or_na(prf_g['bestF1_thr'])} | \"\n",
    "            f\"F1_macro={_fmt_or_na(prf_g['bestF1']['f1_macro'])} | \"\n",
    "            f\"F1_micro={_fmt_or_na(prf_g['bestF1']['f1_micro'])}\"\n",
    "        )\n",
    "\n",
    "    # 1) Warm/Cold ç”¨æˆ·\n",
    "    if 'train_df' in globals():\n",
    "        train_user_set = set(pd.Series(train_df['user_idx']).unique())\n",
    "        warm_user_mask = np.array([uid in train_user_set for uid in all_user_idx], dtype=bool)\n",
    "        cold_user_mask = ~warm_user_mask\n",
    "        print_group(\"Warmç”¨æˆ·\", warm_user_mask)\n",
    "        print_group(\"Coldç”¨æˆ·\", cold_user_mask)\n",
    "    else:\n",
    "        print(\"  âš ï¸ ç¼ºå°‘ train_dfï¼Œè·³è¿‡ Warm/Cold ç”¨æˆ·åˆ†æã€‚\")\n",
    "\n",
    "    # 2) Warm/Cold èµ„äº§\n",
    "    if 'train_df' in globals():\n",
    "        train_asset_set = set(pd.Series(train_df['asset_idx']).unique())\n",
    "        warm_asset_mask = np.array([aid in train_asset_set for aid in all_asset_idx], dtype=bool)\n",
    "        cold_asset_mask = ~warm_asset_mask\n",
    "        print_group(\"Warmèµ„äº§\", warm_asset_mask)\n",
    "        print_group(\"Coldèµ„äº§\", cold_asset_mask)\n",
    "    else:\n",
    "        print(\"  âš ï¸ ç¼ºå°‘ train_dfï¼Œè·³è¿‡ Warm/Cold èµ„äº§åˆ†æã€‚\")\n",
    "\n",
    "    # 3) çƒ­é—¨èµ„äº§ï¼ˆæŒ‰è®­ç»ƒé›†å‡ºç°æ¬¡æ•° Top10%ï¼‰\n",
    "    if 'train_df' in globals():\n",
    "        asset_popularity = pd.Series(train_df['asset_idx']).value_counts().to_dict()\n",
    "        asset_pop_values = np.array([asset_popularity.get(aid, 0) for aid in all_asset_idx])\n",
    "\n",
    "        if asset_pop_values.max() > 0:\n",
    "            pos_mask = (asset_pop_values > 0)\n",
    "            if pos_mask.any():\n",
    "                pop_q90 = np.percentile(asset_pop_values[pos_mask], 90)\n",
    "                hot_asset_mask = (asset_pop_values >= pop_q90)\n",
    "                normal_asset_mask = (asset_pop_values > 0) & (asset_pop_values < pop_q90)\n",
    "                print_group(\"çƒ­é—¨èµ„äº§Top10%\", hot_asset_mask)\n",
    "                print_group(\"æ™®é€šèµ„äº§\", normal_asset_mask)\n",
    "            else:\n",
    "                print(\"  çƒ­é—¨èµ„äº§åˆ†æï¼šéªŒè¯é›†ä¸­èµ„äº§ä»æœªåœ¨è®­ç»ƒå‡ºç°ï¼Œè·³è¿‡ã€‚\")\n",
    "        else:\n",
    "            print(\"  çƒ­é—¨èµ„äº§åˆ†æï¼šéªŒè¯é›†ä¸­èµ„äº§çƒ­åº¦ä¸º 0ï¼Œè·³è¿‡ã€‚\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ ç¼ºå°‘ train_dfï¼Œè·³è¿‡çƒ­é—¨åº¦åˆ†ç»„ã€‚\")\n",
    "\n",
    "    # 4) å…±ç°åˆ†æï¼šè®­ç»ƒé›†è§è¿‡çš„ (user, asset) å¯¹\n",
    "    if 'train_df' in globals():\n",
    "        train_pairs = set(zip(train_df['user_idx'], train_df['asset_idx']))\n",
    "        seen_pair_mask = np.array([(uid, aid) in train_pairs for uid, aid in zip(all_user_idx, all_asset_idx)], dtype=bool)\n",
    "        unseen_pair_mask = ~seen_pair_mask\n",
    "        print_group(\"è®­ç»ƒé›†è§è¿‡çš„(user,asset)å¯¹\", seen_pair_mask)\n",
    "        print_group(\"è®­ç»ƒé›†æœªè§çš„(user,asset)å¯¹\", unseen_pair_mask)\n",
    "    else:\n",
    "        print(\"  âš ï¸ ç¼ºå°‘ train_dfï¼Œè·³è¿‡å…±ç°åˆ†æã€‚\")\n",
    "\n",
    "    # ---- ç»˜å›¾å¹¶ä¿å­˜ï¼ˆROC / PRï¼‰ï¼Œä»…åœ¨ä¸¤ç±»éƒ½å­˜åœ¨æ—¶ç»˜åˆ¶ ----\n",
    "    pos_all, neg_all = _class_counts(y_true)\n",
    "    both_all = (pos_all > 0 and neg_all > 0)\n",
    "\n",
    "    if both_all:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"ROC (AUC = {roc_auc:.4f})\")\n",
    "        plt.plot([0,1],[0,1],'--',lw=1.2)\n",
    "        plt.xlim([0,1]); plt.ylim([0,1.05])\n",
    "        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"Receiver Operating Characteristic\")\n",
    "        plt.legend(loc=\"lower right\"); plt.grid(alpha=0.3)\n",
    "        plt.tight_layout(); plt.savefig(\"xdeepfm_ROC.png\", dpi=150); plt.close()\n",
    "        print(\"\\n[Step3B] âœ… å·²ä¿å­˜ï¼šxdeepfm_ROC.png\")\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "        ap = average_precision_score(y_true, y_score)\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, lw=2, label=f\"PR (AP = {ap:.4f})\")\n",
    "        plt.xlim([0,1]); plt.ylim([0,1.05])\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precisionâ€“Recall Curve\")\n",
    "        plt.legend(loc=\"lower left\"); plt.grid(alpha=0.3)\n",
    "        plt.tight_layout(); plt.savefig(\"xdeepfm_PR.png\", dpi=150); plt.close()\n",
    "        print(\"[Step3B] âœ… å·²ä¿å­˜ï¼šxdeepfm_PR.png\")\n",
    "    else:\n",
    "        print(\"\\n[Step3B] âš ï¸ éªŒè¯é›†åªæœ‰å•ä¸€ç±»åˆ«ï¼Œè·³è¿‡ ROC/PR æ›²çº¿ã€‚\")\n",
    "\n",
    "    # ---- ä¿å­˜ä¸­é—´ç»“æœï¼Œåç»­ç”»å›¾æ— éœ€å†æ¨ç† ----\n",
    "    np.save(\"valid_y_true.npy\",  y_true)\n",
    "    np.save(\"valid_y_score.npy\", y_score)\n",
    "    print(\"[Step3B] ç¼“å­˜ï¼švalid_y_true.npy / valid_y_score.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facaeec1-9de2-4aaa-8cf9-cf860d0a18a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Step3C] å¼€å§‹æŒ‰ç”¨æˆ·çš„æ’åºæŒ‡æ ‡è¯„ä¼°ï¼ˆåŸºäºéªŒè¯é›†ä¸­å·²æœ‰å€™é€‰ï¼‰...\n",
      "[Step3C] Overall AUC = 0.8988\n",
      "[Step3C] Users with >=1 positive in valid = 10968\n",
      "[Step3C] Hit@5 = 0.9957\n",
      "[Step3C] Hit@10 = 0.9995\n",
      "[Step3C] MRR = 0.9217\n",
      "[Step3C] NDCG@5 = 0.9195\n",
      "[Step3C] NDCG@10 = 0.9286\n",
      "[Step3C] å®Œæˆï¼ˆä»…ä½œç ”ç©¶è§‚å¯Ÿï¼Œä¸å½±å“è®­ç»ƒ/å¯¼å‡ºï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 3Cï¼šæŒ‰ç”¨æˆ·çš„æ’åºæŒ‡æ ‡ï¼ˆHit@K / MRR / NDCG@Kï¼‰\n",
    "# ä¾èµ–ï¼šmodel, DEVICE, USE_AMP, valid_dataset\n",
    "# ================================\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "K_LIST = [5, 10]  # å¯æ”¹ä¸º [5, 10, 20]\n",
    "\n",
    "def make_eval_loader_for_userwise(dataset):\n",
    "    num_workers = 4 if USE_AMP else 0\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=(2048 if USE_AMP else 512),\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if USE_AMP else False,\n",
    "        persistent_workers=True if (USE_AMP and num_workers>0) else False,\n",
    "        prefetch_factor=(2 if (USE_AMP and num_workers>0) else None),\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_valid_arrays():\n",
    "    loader = make_eval_loader_for_userwise(valid_dataset)\n",
    "    model.eval().to(DEVICE)\n",
    "    users, items, labels, scores = [], [], [], []\n",
    "    with torch.inference_mode():\n",
    "        for batch in loader:\n",
    "            u, ucat, a, acat, anum, y = batch\n",
    "            u_d = u.to(DEVICE, non_blocking=USE_AMP)\n",
    "            ucat_d = ucat.to(DEVICE, non_blocking=USE_AMP)\n",
    "            a_d = a.to(DEVICE, non_blocking=USE_AMP)\n",
    "            acat_d = acat.to(DEVICE, non_blocking=USE_AMP)\n",
    "            anum_d = anum.to(DEVICE, non_blocking=USE_AMP)\n",
    "            with torch.amp.autocast(device_type=DEVICE.type, enabled=USE_AMP):\n",
    "                logit = model(u_d, ucat_d, a_d, acat_d, anum_d)\n",
    "                prob = torch.sigmoid(logit).float().detach().cpu().numpy()\n",
    "            users.append(u.cpu().numpy()); items.append(a.cpu().numpy())\n",
    "            labels.append(y.cpu().numpy()); scores.append(prob)\n",
    "    users = np.concatenate(users)\n",
    "    items = np.concatenate(items)\n",
    "    labels = np.concatenate(labels).astype(np.int64)\n",
    "    scores = np.concatenate(scores).astype(np.float32)\n",
    "    return users, items, labels, scores\n",
    "\n",
    "def dcg_at_k(rel_sorted, k):\n",
    "    \"\"\"æ”¯æŒæ ·æœ¬æ•°å°‘äº k çš„æƒ…å†µ\"\"\"\n",
    "    if rel_sorted.size == 0:\n",
    "        return 0.0\n",
    "    eff_k = min(k, rel_sorted.size)\n",
    "    rel = rel_sorted[:eff_k]\n",
    "    denom = np.log2(np.arange(2, eff_k + 2))\n",
    "    return float(np.sum((2.0**rel - 1.0) / denom))\n",
    "\n",
    "def ndcg_at_k(rel_sorted, k):\n",
    "    \"\"\"æ”¯æŒæ ·æœ¬æ•°å°‘äº k çš„æƒ…å†µ\"\"\"\n",
    "    if rel_sorted.size == 0:\n",
    "        return 0.0\n",
    "    eff_k = min(k, rel_sorted.size)\n",
    "    ideal = np.sort(rel_sorted)[::-1][:eff_k]\n",
    "    # è¿™é‡ŒæŠŠ k æ¢æˆ eff_k ä¼ ç»™ dcgï¼Œç¡®ä¿åˆ†å­åˆ†æ¯åŒé•¿åº¦\n",
    "    return dcg_at_k(rel_sorted, eff_k) / max(dcg_at_k(ideal, eff_k), 1e-9)\n",
    "\n",
    "def compute_userwise_metrics(users, items, labels, scores, K_list=(5,10)):\n",
    "    # æŒ‰ç”¨æˆ·èšåˆå€™é€‰ï¼ˆä»…éªŒè¯é›†ä¸­å·²æœ‰çš„æ ·æœ¬ï¼‰\n",
    "    by_user = defaultdict(list)\n",
    "    for u, it, y, s in zip(users, items, labels, scores):\n",
    "        by_user[int(u)].append((int(it), int(y), float(s)))\n",
    "\n",
    "    hits = {k: [] for k in K_list}\n",
    "    mrrs = []\n",
    "    ndcgs = {k: [] for k in K_list}\n",
    "\n",
    "    valid_users = 0\n",
    "    for u, lst in by_user.items():\n",
    "        # è¯¥ç”¨æˆ·åœ¨éªŒè¯é›†é‡Œå¦‚æœæ²¡æœ‰ä»»ä½•æ­£ä¾‹ï¼Œå°±è·³è¿‡\n",
    "        if not any(y==1 for _, y, _ in lst):\n",
    "            continue\n",
    "        valid_users += 1\n",
    "\n",
    "        # æŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½æ’åº\n",
    "        lst_sorted = sorted(lst, key=lambda x: x[2], reverse=True)\n",
    "        rel = np.array([y for _, y, _ in lst_sorted], dtype=np.float32)\n",
    "\n",
    "        # Hit@Kï¼ˆè€ƒè™‘æ ·æœ¬æ•°å°‘äº kï¼‰\n",
    "        for k in K_list:\n",
    "            eff_k = min(k, rel.size)\n",
    "            hit = float(rel[:eff_k].max()) if eff_k > 0 else 0.0\n",
    "            hits[k].append(hit)\n",
    "\n",
    "        # MRRï¼šç¬¬ä¸€ä¸ªæ­£ä¾‹çš„å€’æ•°æ’å\n",
    "        rr = 0.0\n",
    "        for rank, y in enumerate(rel, start=1):\n",
    "            if y > 0.5:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        mrrs.append(rr)\n",
    "\n",
    "        # NDCG@Kï¼ˆå†…éƒ¨å·²å¤„ç† eff_kï¼‰\n",
    "        for k in K_list:\n",
    "            ndcgs[k].append(ndcg_at_k(rel, k))\n",
    "\n",
    "    out = {\n",
    "        \"num_users_with_positive\": valid_users,\n",
    "        \"Hit@K\": {k: float(np.mean(hits[k])) if hits[k] else float(\"nan\") for k in K_list},\n",
    "        \"MRR\": float(np.mean(mrrs)) if mrrs else float(\"nan\"),\n",
    "        \"NDCG@K\": {k: float(np.mean(ndcgs[k])) if ndcgs[k] else float(\"nan\") for k in K_list},\n",
    "    }\n",
    "    return out\n",
    "\n",
    "# â€”â€” æ‰§è¡Œ â€”â€” #\n",
    "print(\"\\n[Step3C] å¼€å§‹æŒ‰ç”¨æˆ·çš„æ’åºæŒ‡æ ‡è¯„ä¼°ï¼ˆåŸºäºéªŒè¯é›†ä¸­å·²æœ‰å€™é€‰ï¼‰...\")\n",
    "u_all, i_all, y_all, s_all = collect_valid_arrays()\n",
    "auc_overall = roc_auc_score(y_all, s_all) if (y_all.sum()>0 and (1-y_all).sum()>0) else float(\"nan\")\n",
    "userwise = compute_userwise_metrics(u_all, i_all, y_all, s_all, K_list=K_LIST)\n",
    "\n",
    "print(f\"[Step3C] Overall AUC = {auc_overall:.4f}\")\n",
    "print(f\"[Step3C] Users with >=1 positive in valid = {userwise['num_users_with_positive']}\")\n",
    "for k, v in userwise[\"Hit@K\"].items():\n",
    "    print(f\"[Step3C] Hit@{k} = {v:.4f}\")\n",
    "print(f\"[Step3C] MRR = {userwise['MRR']:.4f}\")\n",
    "for k, v in userwise[\"NDCG@K\"].items():\n",
    "    print(f\"[Step3C] NDCG@{k} = {v:.4f}\")\n",
    "print(\"[Step3C] å®Œæˆï¼ˆä»…ä½œç ”ç©¶è§‚å¯Ÿï¼Œä¸å½±å“è®­ç»ƒ/å¯¼å‡ºï¼‰ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2047a5f8-582c-4e40-833f-aedab897fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ç ”ç©¶ç‰ˆ] è¿‡æ‹Ÿåˆåˆ†æï¼š\n",
      "  æœ€ç»ˆLosså·®è·: 0.19487\n",
      "  è¯Šæ–­: ä¸¥é‡è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–\n",
      "\n",
      "[ç ”ç©¶ç‰ˆ] è®­ç»ƒæ‘˜è¦ï¼š\n",
      "  æœ€ä½³AUC: 0.89877 @ Epoch 2\n",
      "  æœ€ç»ˆAUC: 0.87766\n",
      "  æç¤º: æœ€ç»ˆæ¨¡å‹ä¸æ˜¯æœ€ä¼˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ\n",
      "  æœ€ä½³LogLoss: 0.46923 @ Epoch 4\n",
      "  æœ€ç»ˆLogLoss: 0.48301\n",
      "\n",
      "è®­ç»ƒæ›²çº¿å·²ä¿å­˜ï¼š xdeepfm_training_loss.png / xdeepfm_training_auc.png / xdeepfm_training_logloss.png / xdeepfm_training_overfitting.png / xdeepfm_training_combined.png\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 4A: è®­ç»ƒæ›²çº¿å¯è§†åŒ–ï¼ˆLoss / AUC / Loglossï¼‰ã€ç ”ç©¶ç‰ˆã€‘\n",
    "# ä¾èµ–: history = {\"train_loss\": [], \"valid_loss\": [], \"valid_auc\": [], \"valid_logloss\": []}\n",
    "# å­¦ä¹ ç‡æ›²çº¿ã€è¿‡æ‹Ÿåˆç¨‹åº¦åˆ†æ\n",
    "# ================================\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def _clean_series(y: List[float]) -> np.ndarray:\n",
    "    \"\"\"æŠŠåˆ—è¡¨è½¬æˆ ndarrayï¼Œå¹¶å»æ‰å¤´å°¾çš„ NaNï¼ˆä¸­é—´ NaN ä¿ç•™ï¼Œé¿å…é”™ä½ï¼‰\"\"\"\n",
    "    if y is None:\n",
    "        return np.array([])\n",
    "    arr = np.asarray(y, dtype=float)\n",
    "    if arr.size == 0:\n",
    "        return arr\n",
    "    if np.isnan(arr).all():\n",
    "        return np.array([])\n",
    "    # å»æ‰å‰å NaNï¼Œä¿ç•™ä¸­é—´ NaN\n",
    "    first = np.argmax(~np.isnan(arr))\n",
    "    last = len(arr) - np.argmax(~np.isnan(arr[::-1]))\n",
    "    return arr[first:last]\n",
    "\n",
    "def _plot_xy(x: np.ndarray, ys: List[Tuple[np.ndarray, str]], title: str, ylabel: str,\n",
    "             save_path: Path, ylim: Tuple[float, float] = None):\n",
    "    \"\"\"é€šç”¨ç”»å›¾ï¼šæ”¯æŒå•ç‚¹ï¼Œç”¨ markerï¼Œè‡ªåŠ¨ç½‘æ ¼ä¸ç´§å‡‘å¸ƒå±€\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    drew = False\n",
    "    for y, label in ys:\n",
    "        if y.size == 0:\n",
    "            continue\n",
    "        n = min(x.size, y.size)\n",
    "        if n == 0:\n",
    "            continue\n",
    "        xx, yy = x[:n], y[:n]\n",
    "        plt.plot(xx, yy, label=label, marker='o', linewidth=1.5, markersize=4)\n",
    "        drew = True\n",
    "\n",
    "    if not drew:\n",
    "        plt.close()\n",
    "        return False\n",
    "\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(str(save_path), dpi=150)\n",
    "    plt.close()\n",
    "    return True\n",
    "\n",
    "def plot_training_curves(history: dict, save_prefix=\"xdeepfm_training\", save_dir: str = \".\"):\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # å–å‡ºå¹¶æ¸…æ´—\n",
    "    tl  = _clean_series(history.get(\"train_loss\"))\n",
    "    vl  = _clean_series(history.get(\"valid_loss\"))\n",
    "    va  = _clean_series(history.get(\"valid_auc\"))\n",
    "    vll = _clean_series(history.get(\"valid_logloss\"))\n",
    "\n",
    "    # ç»Ÿä¸€ x è½´ï¼ˆä»¥æœ€é•¿çš„ä¸€ä¸ªä¸ºå‡†ï¼‰\n",
    "    max_len = max([arr.size for arr in [tl, vl, va, vll]] + [0])\n",
    "    if max_len == 0:\n",
    "        print(\"âš ï¸ history ä¸ºç©ºæˆ–å…¨ NaNï¼Œæœªç”Ÿæˆä»»ä½•æ›²çº¿ã€‚\")\n",
    "        return\n",
    "\n",
    "    epochs = np.arange(1, max_len + 1)\n",
    "    saved = []\n",
    "\n",
    "    # 1) Lossæ›²çº¿\n",
    "    out_loss = save_dir / f\"{save_prefix}_loss.png\"\n",
    "    if _plot_xy(epochs,\n",
    "                [(tl, \"Train Loss\"), (vl, \"Valid Loss\")],\n",
    "                title=\"Training vs Validation Loss\",\n",
    "                ylabel=\"Loss\",\n",
    "                save_path=out_loss):\n",
    "        saved.append(out_loss.name)\n",
    "\n",
    "    # 2) AUCæ›²çº¿\n",
    "    out_auc = save_dir / f\"{save_prefix}_auc.png\"\n",
    "    if _plot_xy(epochs,\n",
    "                [(va, \"Valid AUC\")],\n",
    "                title=\"Validation AUC\",\n",
    "                ylabel=\"AUC\",\n",
    "                save_path=out_auc,\n",
    "                ylim=(0.0, 1.0)):\n",
    "        saved.append(out_auc.name)\n",
    "\n",
    "    # 3) Loglossæ›²çº¿\n",
    "    out_logloss = save_dir / f\"{save_prefix}_logloss.png\"\n",
    "    if vll.size > 0 and _plot_xy(epochs,\n",
    "                                 [(vll, \"Valid Logloss\")],\n",
    "                                 title=\"Validation Logloss\",\n",
    "                                 ylabel=\"Logloss\",\n",
    "                                 save_path=out_logloss):\n",
    "        saved.append(out_logloss.name)\n",
    "\n",
    "    # ================================\n",
    "    # è¯Šæ–­æ€§å¯è§†åŒ–\n",
    "    # ================================\n",
    "    \n",
    "    # 4) è¿‡æ‹Ÿåˆç¨‹åº¦åˆ†æï¼ˆTrain vs Valid Losså·®å¼‚ï¼‰\n",
    "    if tl.size > 0 and vl.size > 0:\n",
    "        n = min(tl.size, vl.size)\n",
    "        overfitting_gap = vl[:n] - tl[:n]\n",
    "        \n",
    "        out_overfit = save_dir / f\"{save_prefix}_overfitting.png\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(epochs[:n], overfitting_gap, 'r-o', label='Valid Loss - Train Loss', \n",
    "                linewidth=1.5, markersize=4)\n",
    "        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss Gap\")\n",
    "        plt.title(\"Overfitting Analysis (Gap = Valid - Train)\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(out_overfit), dpi=150)\n",
    "        plt.close()\n",
    "        saved.append(out_overfit.name)\n",
    "        \n",
    "        # æ‰“å°è¿‡æ‹Ÿåˆè¯Šæ–­\n",
    "        final_gap = overfitting_gap[-1] if n > 0 else 0\n",
    "        print(f\"\\n[ç ”ç©¶ç‰ˆ] è¿‡æ‹Ÿåˆåˆ†æï¼š\")\n",
    "        print(f\"  æœ€ç»ˆLosså·®è·: {final_gap:.5f}\")\n",
    "        if final_gap < 0.01:\n",
    "            print(\"  è¯Šæ–­: åŸºæœ¬æ— è¿‡æ‹Ÿåˆ\")\n",
    "        elif final_gap < 0.05:\n",
    "            print(\"  è¯Šæ–­: è½»åº¦è¿‡æ‹Ÿåˆ\")\n",
    "        elif final_gap < 0.1:\n",
    "            print(\"  è¯Šæ–­: ä¸­åº¦è¿‡æ‹Ÿåˆ\")\n",
    "        else:\n",
    "            print(\"  è¯Šæ–­: ä¸¥é‡è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–\")\n",
    "    \n",
    "    # 5) ç»„åˆè¯Šæ–­å›¾ï¼ˆ2x2å­å›¾ï¼‰\n",
    "    if any([tl.size > 0, vl.size > 0, va.size > 0, vll.size > 0]):\n",
    "        out_combined = save_dir / f\"{save_prefix}_combined.png\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        fig.suptitle('Training Progress Overview', fontsize=16)\n",
    "        \n",
    "        # å­å›¾1: Loss\n",
    "        ax1 = axes[0, 0]\n",
    "        if tl.size > 0:\n",
    "            ax1.plot(epochs[:tl.size], tl, 'b-o', label='Train Loss', markersize=4)\n",
    "        if vl.size > 0:\n",
    "            ax1.plot(epochs[:vl.size], vl, 'r-o', label='Valid Loss', markersize=4)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss Curves')\n",
    "        ax1.grid(alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # å­å›¾2: AUC\n",
    "        ax2 = axes[0, 1]\n",
    "        if va.size > 0:\n",
    "            ax2.plot(epochs[:va.size], va, 'g-o', label='Valid AUC', markersize=4)\n",
    "            ax2.set_ylim(0.0, 1.0)\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('AUC')\n",
    "        ax2.set_title('Validation AUC')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # å­å›¾3: LogLoss\n",
    "        ax3 = axes[1, 0]\n",
    "        if vll.size > 0:\n",
    "            ax3.plot(epochs[:vll.size], vll, 'm-o', label='Valid LogLoss', markersize=4)\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('LogLoss')\n",
    "        ax3.set_title('Validation LogLoss')\n",
    "        ax3.grid(alpha=0.3)\n",
    "        ax3.legend()\n",
    "        \n",
    "        # å­å›¾4: æ—©åœæŒ‡æ ‡ï¼ˆå‡è®¾ç”¨AUCï¼‰\n",
    "        ax4 = axes[1, 1]\n",
    "        if va.size > 0:\n",
    "            # æ ‡è®°æœ€ä½³epoch\n",
    "            best_epoch = np.argmax(va) + 1\n",
    "            ax4.plot(epochs[:va.size], va, 'b-o', markersize=4)\n",
    "            ax4.plot(best_epoch, va[best_epoch-1], 'r*', markersize=15, \n",
    "                    label=f'Best @ Epoch {best_epoch}')\n",
    "            ax4.set_ylim(0.0, 1.0)\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Metric')\n",
    "        ax4.set_title('Early Stopping Monitor (AUC)')\n",
    "        ax4.grid(alpha=0.3)\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(out_combined), dpi=150)\n",
    "        plt.close()\n",
    "        saved.append(out_combined.name)\n",
    "    \n",
    "    # æ‰“å°è®­ç»ƒæ‘˜è¦\n",
    "    print(f\"\\n[ç ”ç©¶ç‰ˆ] è®­ç»ƒæ‘˜è¦ï¼š\")\n",
    "    if va.size > 0:\n",
    "        best_auc_epoch = np.argmax(va) + 1\n",
    "        print(f\"  æœ€ä½³AUC: {va[best_auc_epoch-1]:.5f} @ Epoch {best_auc_epoch}\")\n",
    "        print(f\"  æœ€ç»ˆAUC: {va[-1]:.5f}\")\n",
    "        if va[-1] < va[best_auc_epoch-1] - 0.001:\n",
    "            print(\"  æç¤º: æœ€ç»ˆæ¨¡å‹ä¸æ˜¯æœ€ä¼˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ\")\n",
    "    \n",
    "    if vll.size > 0:\n",
    "        best_ll_epoch = np.argmin(vll) + 1\n",
    "        print(f\"  æœ€ä½³LogLoss: {vll[best_ll_epoch-1]:.5f} @ Epoch {best_ll_epoch}\")\n",
    "        print(f\"  æœ€ç»ˆLogLoss: {vll[-1]:.5f}\")\n",
    "\n",
    "    if saved:\n",
    "        print(f\"\\nè®­ç»ƒæ›²çº¿å·²ä¿å­˜ï¼š {' / '.join(saved)}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æŒ‡æ ‡ä¸å¯ç”¨ï¼Œæœªç”Ÿæˆä»»ä½•æ›²çº¿ã€‚\")\n",
    "\n",
    "# è°ƒç”¨\n",
    "plot_training_curves(history, save_prefix=\"xdeepfm_training\", save_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9a35261-f81f-4b2c-84d1-c25bc5bb1cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step4B ç ”ç©¶ç‰ˆ] åŠ è½½æœ€ä½³éªŒè¯æ¨¡å‹æƒé‡ï¼šxdeepfm_best.pt\n",
      "[Step4B ç ”ç©¶ç‰ˆ] çŸ©é˜µè¡¨å¤´å†™å…¥ï¼š807 åˆ—ï¼ˆå« customerIDï¼‰\n",
      "[Step4B ç ”ç©¶ç‰ˆ] å¼€å§‹æŒ‰ç”¨æˆ·æ‰¹æ¬¡ç”Ÿæˆã€å®½çŸ©é˜µã€‘æ¦‚ç‡åˆ†æ•°â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ç”¨æˆ·æ‰¹æ¬¡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:32<00:00,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step4B ç ”ç©¶ç‰ˆ] å®½çŸ©é˜µå·²å¯¼å‡ºåˆ°ï¼šuser_asset_preferences_full.csv\n",
      "å…±å†™å…¥ç”¨æˆ· 29,090ï¼ŒçŸ©é˜µç»´åº¦çº¦ä¸º (29,090, 807)ï¼ˆå« customerID åˆ—ï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 4B ç ”ç©¶ç‰ˆï¼šå…¨é‡åå¥½çŸ©é˜µå¯¼å‡ºï¼ˆæ— å…¨é‡å¾®è°ƒï¼‰\n",
    "# - ç›´æ¥åŠ è½½ BEST_PATH æƒé‡\n",
    "# - å¯¼å‡ºä¸ºâ€œå®½çŸ©é˜µâ€ï¼šç¬¬ä¸€åˆ— customerIDï¼Œç¬¬ä¸€è¡Œ ISIN\n",
    "# - ä¿æŒ GPU è‡ªåŠ¨åŠ é€Ÿ / CPU å…¼å®¹\n",
    "# ä¾èµ–ï¼š\n",
    "# - model, BEST_PATH, user2idx, asset2idx\n",
    "# - user_cat_features, asset_cat_features, num_cols\n",
    "# - customers_latest, assets_latest\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_AMP = (DEVICE.type == \"cuda\")\n",
    "BF16_OK = (USE_AMP and torch.cuda.is_bf16_supported())\n",
    "AMP_DTYPE = torch.bfloat16 if BF16_OK else torch.float16\n",
    "\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡\n",
    "if os.path.exists(BEST_PATH):\n",
    "    state = torch.load(BEST_PATH, map_location=\"cpu\")\n",
    "    model.load_state_dict(state if isinstance(state, dict) else state.state_dict())\n",
    "    print(f\"[Step4B ç ”ç©¶ç‰ˆ] åŠ è½½æœ€ä½³éªŒè¯æ¨¡å‹æƒé‡ï¼š{BEST_PATH}\")\n",
    "else:\n",
    "    print(f\"[Step4B ç ”ç©¶ç‰ˆ] WARNINGï¼šæœªæ‰¾åˆ° {BEST_PATH}ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹æƒé‡\")\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# åˆ†å—å¯¼å‡ºé…ç½®ï¼ˆæŒ‰è®¾å¤‡ï¼‰\n",
    "USER_BATCH_GPU = 256\n",
    "ITEM_BATCH_GPU = 256\n",
    "USER_BATCH_CPU = 128\n",
    "ITEM_BATCH_CPU = 128\n",
    "\n",
    "USER_BATCH = USER_BATCH_GPU if USE_AMP else USER_BATCH_CPU\n",
    "ITEM_BATCH = ITEM_BATCH_GPU if USE_AMP else ITEM_BATCH_CPU\n",
    "\n",
    "OUT_CSV = \"user_asset_preferences_full.csv\"\n",
    "\n",
    "# åå‘ç´¢å¼•\n",
    "idx2user = {v: k for k, v in user2idx.items()}\n",
    "idx2asset = {v: k for k, v in asset2idx.items()}\n",
    "\n",
    "# é¢„å–ç‰¹å¾è¡¨ï¼ˆç¡®ä¿æŒ‰ idx å¯ .locï¼‰\n",
    "all_users_cat = customers_latest.set_index('user_idx')[user_cat_features].sort_index()\n",
    "all_assets_cat_num = assets_latest.set_index('asset_idx')[asset_cat_features + num_cols].sort_index()\n",
    "\n",
    "all_user_indices = np.arange(len(user2idx))\n",
    "all_asset_indices = np.arange(len(asset2idx))\n",
    "\n",
    "# ================================\n",
    "# å°†â€œé•¿è¡¨â€å¯¼å‡ºæ”¹ä¸ºâ€œå®½çŸ©é˜µâ€å¯¼å‡ºï¼ˆç¬¬ä¸€åˆ—ç”¨æˆ·ï¼Œç¬¬ä¸€è¡Œèµ„äº§ï¼‰\n",
    "# ================================\n",
    "\n",
    "# åˆ—è½´ï¼šæŒ‰ asset_idx é¡ºåºå–å‡ºæ‰€æœ‰ ISIN\n",
    "all_isins = [idx2asset[int(i)] for i in all_asset_indices]\n",
    "\n",
    "# åˆ é™¤æ—§æ–‡ä»¶ï¼Œå¹¶å†™å…¥è¡¨å¤´ï¼ˆcustomerID + æ‰€æœ‰ ISINï¼‰\n",
    "if os.path.exists(OUT_CSV):\n",
    "    os.remove(OUT_CSV)\n",
    "\n",
    "header = [\"customerID\"] + all_isins\n",
    "pd.DataFrame([header]).to_csv(OUT_CSV, index=False, header=False)\n",
    "print(f\"[Step4B ç ”ç©¶ç‰ˆ] çŸ©é˜µè¡¨å¤´å†™å…¥ï¼š{len(header)} åˆ—ï¼ˆå« customerIDï¼‰\")\n",
    "\n",
    "total_users_written = 0\n",
    "print(\"[Step4B ç ”ç©¶ç‰ˆ] å¼€å§‹æŒ‰ç”¨æˆ·æ‰¹æ¬¡ç”Ÿæˆã€å®½çŸ©é˜µã€‘æ¦‚ç‡åˆ†æ•°â€¦\")\n",
    "\n",
    "# ç”¨ inference_mode æ¯” no_grad æ›´çœåŒæ­¥\n",
    "with torch.inference_mode():\n",
    "    for ui in tqdm(range(0, len(all_user_indices), USER_BATCH), desc=\"ç”¨æˆ·æ‰¹æ¬¡\"):\n",
    "        # å–ä¸€æ‰¹ç”¨æˆ·\n",
    "        u_idx_batch = all_user_indices[ui: ui + USER_BATCH].tolist()\n",
    "        U = len(u_idx_batch)\n",
    "\n",
    "        # (U,) ç”¨æˆ·ç´¢å¼•ä¸ç‰¹å¾ï¼ˆæ”¾åœ¨ DEVICE ä¸Šï¼‰\n",
    "        user_batch_idx = torch.tensor(u_idx_batch, dtype=torch.long, device=DEVICE)\n",
    "        user_cat_batch = torch.tensor(\n",
    "            all_users_cat.loc[u_idx_batch].values, dtype=torch.long, device=DEVICE\n",
    "        )  # (U, num_user_cat)\n",
    "\n",
    "        # ä¸ºäº†çœæ˜¾å­˜ï¼šå…¨èµ„äº§æŒ‰ ITEM_BATCH åˆ†å—æ¨ç†ï¼Œæ¨ªå‘æ‹¼æ¥\n",
    "        probs_blocks = []\n",
    "        for ai in range(0, len(all_asset_indices), ITEM_BATCH):\n",
    "            a_idx_batch = all_asset_indices[ai: ai + ITEM_BATCH].tolist()\n",
    "            A = len(a_idx_batch)\n",
    "\n",
    "            asset_batch_idx = torch.tensor(a_idx_batch, dtype=torch.long, device=DEVICE)\n",
    "            asset_cat_batch = torch.tensor(\n",
    "                all_assets_cat_num.loc[a_idx_batch][asset_cat_features].values,\n",
    "                dtype=torch.long, device=DEVICE\n",
    "            )  # (A, num_asset_cat)\n",
    "            asset_num_batch = torch.tensor(\n",
    "                all_assets_cat_num.loc[a_idx_batch][num_cols].values,\n",
    "                dtype=torch.float32, device=DEVICE\n",
    "            )  # (A, num_num_cols)\n",
    "\n",
    "            # ç»„è£…æˆ UÃ—A å¯¹é½çš„æ‰å¹³è¾“å…¥ï¼Œç„¶åå† reshape å› (U, A)\n",
    "            user_expand_idx  = user_batch_idx.unsqueeze(1).repeat(1, A).reshape(-1)               # (U*A,)\n",
    "            asset_expand_idx = asset_batch_idx.unsqueeze(0).repeat(U, 1).reshape(-1)              # (U*A,)\n",
    "            user_cat_expand  = user_cat_batch.unsqueeze(1).repeat(1, A, 1).reshape(-1, user_cat_batch.size(1))  # (U*A, _)\n",
    "            asset_cat_expand = asset_cat_batch.unsqueeze(0).repeat(U, 1, 1).reshape(-1, asset_cat_batch.size(1)) # (U*A, _)\n",
    "            asset_num_expand = asset_num_batch.unsqueeze(0).repeat(U, 1, 1).reshape(-1, asset_num_batch.size(1)) # (U*A, _)\n",
    "\n",
    "            # AMP ä¸‹å¯èƒ½æ˜¯ BF16ï¼Œè½¬æˆ FP32 å†è½ç›˜\n",
    "            with torch.amp.autocast(device_type=DEVICE.type, enabled=USE_AMP, dtype=AMP_DTYPE):\n",
    "                logits = model(user_expand_idx, user_cat_expand, asset_expand_idx, asset_cat_expand, asset_num_expand)\n",
    "                probs  = torch.sigmoid(logits).to(torch.float32)\n",
    "\n",
    "            probs_chunk = probs.detach().cpu().numpy().reshape(U, A)  # è¿˜åŸæˆ (U, A_chunk)\n",
    "            probs_blocks.append(probs_chunk)\n",
    "\n",
    "        # æ¨ªå‘æ‹¼æ¥æ‰€æœ‰èµ„äº§å— -> (U, |all_assets|)\n",
    "        probs_full = np.concatenate(probs_blocks, axis=1).astype(np.float32)\n",
    "\n",
    "        # ç”Ÿæˆæœ¬æ‰¹ DataFrameï¼ˆæŒ‰â€œå®½çŸ©é˜µâ€ç»“æ„ï¼‰ï¼Œç¬¬ä¸€åˆ— customerID\n",
    "        df_chunk = pd.DataFrame(probs_full, columns=all_isins)\n",
    "        df_chunk.insert(0, \"customerID\", [idx2user[int(i)] for i in u_idx_batch])\n",
    "\n",
    "        # ä»¥è¿½åŠ æ–¹å¼å†™å…¥ï¼ˆè¡¨å¤´å·²å†™è¿‡ï¼Œè¿™é‡Œä¸å†å†™ headerï¼‰\n",
    "        df_chunk.to_csv(OUT_CSV, mode=\"a\", index=False, header=False, float_format=\"%.6f\")\n",
    "\n",
    "        total_users_written += U\n",
    "\n",
    "print(f\"[Step4B ç ”ç©¶ç‰ˆ] å®½çŸ©é˜µå·²å¯¼å‡ºåˆ°ï¼š{OUT_CSV}\")\n",
    "print(f\"å…±å†™å…¥ç”¨æˆ· {total_users_written:,}ï¼ŒçŸ©é˜µç»´åº¦çº¦ä¸º ({total_users_written:,}, {len(all_isins)+1:,})ï¼ˆå« customerID åˆ—ï¼‰\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
